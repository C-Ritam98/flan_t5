{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "pKi_YKIlK5xQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "4Q4hcBEaPQ7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PART-1"
      ],
      "metadata": {
        "id": "4_UsnQHrXLu2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgdVq7J2I3BF"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjSClJnLVMv-",
        "outputId": "05ad867d-a4f0-4ee2-ea16-800e1e75c2fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "T5Config {\n",
              "  \"_name_or_path\": \"google/flan-t5-small\",\n",
              "  \"architectures\": [\n",
              "    \"T5ForConditionalGeneration\"\n",
              "  ],\n",
              "  \"d_ff\": 1024,\n",
              "  \"d_kv\": 64,\n",
              "  \"d_model\": 512,\n",
              "  \"decoder_start_token_id\": 0,\n",
              "  \"dense_act_fn\": \"gelu_new\",\n",
              "  \"dropout_rate\": 0.1,\n",
              "  \"eos_token_id\": 1,\n",
              "  \"feed_forward_proj\": \"gated-gelu\",\n",
              "  \"initializer_factor\": 1.0,\n",
              "  \"is_encoder_decoder\": true,\n",
              "  \"is_gated_act\": true,\n",
              "  \"layer_norm_epsilon\": 1e-06,\n",
              "  \"model_type\": \"t5\",\n",
              "  \"n_positions\": 512,\n",
              "  \"num_decoder_layers\": 8,\n",
              "  \"num_heads\": 6,\n",
              "  \"num_layers\": 8,\n",
              "  \"output_past\": true,\n",
              "  \"pad_token_id\": 0,\n",
              "  \"relative_attention_max_distance\": 128,\n",
              "  \"relative_attention_num_buckets\": 32,\n",
              "  \"task_specific_params\": {\n",
              "    \"summarization\": {\n",
              "      \"early_stopping\": true,\n",
              "      \"length_penalty\": 2.0,\n",
              "      \"max_length\": 200,\n",
              "      \"min_length\": 30,\n",
              "      \"no_repeat_ngram_size\": 3,\n",
              "      \"num_beams\": 4,\n",
              "      \"prefix\": \"summarize: \"\n",
              "    },\n",
              "    \"translation_en_to_de\": {\n",
              "      \"early_stopping\": true,\n",
              "      \"max_length\": 300,\n",
              "      \"num_beams\": 4,\n",
              "      \"prefix\": \"translate English to German: \"\n",
              "    },\n",
              "    \"translation_en_to_fr\": {\n",
              "      \"early_stopping\": true,\n",
              "      \"max_length\": 300,\n",
              "      \"num_beams\": 4,\n",
              "      \"prefix\": \"translate English to French: \"\n",
              "    },\n",
              "    \"translation_en_to_ro\": {\n",
              "      \"early_stopping\": true,\n",
              "      \"max_length\": 300,\n",
              "      \"num_beams\": 4,\n",
              "      \"prefix\": \"translate English to Romanian: \"\n",
              "    }\n",
              "  },\n",
              "  \"tie_word_embeddings\": false,\n",
              "  \"transformers_version\": \"4.31.0\",\n",
              "  \"use_cache\": true,\n",
              "  \"vocab_size\": 32128\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summarisation Task"
      ],
      "metadata": {
        "id": "-r0O1aFLNsvm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(\"\"\"Summarize: Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\"\"\", return_tensors=\"pt\")\n",
        "outputs = model.generate(**inputs)\n",
        "print(tokenizer.batch_decode(outputs, skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujtO1r-ENq7O",
        "outputId": "6b8820d0-2969-4da6-913c-87791bb4ef8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1369: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The main entrance to the school is a sand castle.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Translation Task"
      ],
      "metadata": {
        "id": "L1zSJDzINwzQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(\"translate English to French: A step by step recipe to make bolognese pasta\", return_tensors=\"pt\")\n",
        "outputs = model.generate(**inputs)\n",
        "print(tokenizer.batch_decode(outputs, skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9Fv5eiANwML",
        "outputId": "4067bc99-3daa-4bde-df80-26adb3c6c1a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"Un risqué d'élaboration de l'assa\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q&A Task"
      ],
      "metadata": {
        "id": "kKcScxhmN2bo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(\"\"\"Answer the following question based on the context.\\nContext: Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\\nQuestion: To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\"\"\", return_tensors=\"pt\")\n",
        "outputs = model.generate(**inputs, num_beams = 4, do_sample = True)\n",
        "print(tokenizer.batch_decode(outputs, skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SiOekYJAN0JR",
        "outputId": "72595ab5-9044-4cef-d102-9b2054924174"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Saint Bernadette Soubirous']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = 0\n",
        "total_size = 0\n",
        "for name,param in model.named_parameters():\n",
        "  print(f\"Layer Name: {name} || Tensor size: {param.size()}\")\n",
        "  total_params += param.numel()\n",
        "  total_size += param.numel() * param.element_size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4RT54FMLU7F",
        "outputId": "748ca38c-c9c2-49f3-da14-f4f11791caf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer Name: shared.weight || Tensor size: torch.Size([32128, 512])\n",
            "Layer Name: encoder.block.0.layer.0.SelfAttention.q.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: encoder.block.0.layer.0.SelfAttention.k.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: encoder.block.0.layer.0.SelfAttention.v.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: encoder.block.0.layer.0.SelfAttention.o.weight || Tensor size: torch.Size([512, 384])\n",
            "Layer Name: encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight || Tensor size: torch.Size([32, 6])\n",
            "Layer Name: encoder.block.0.layer.0.layer_norm.weight || Tensor size: torch.Size([512])\n",
            "Layer Name: encoder.block.0.layer.1.DenseReluDense.wi_0.weight || Tensor size: torch.Size([1024, 512])\n",
            "Layer Name: encoder.block.0.layer.1.DenseReluDense.wi_1.weight || Tensor size: torch.Size([1024, 512])\n",
            "Layer Name: encoder.block.0.layer.1.DenseReluDense.wo.weight || Tensor size: torch.Size([512, 1024])\n",
            "Layer Name: encoder.block.0.layer.1.layer_norm.weight || Tensor size: torch.Size([512])\n",
            "Layer Name: encoder.block.1.layer.0.SelfAttention.q.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: encoder.block.1.layer.0.SelfAttention.k.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: encoder.block.1.layer.0.SelfAttention.v.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: encoder.block.1.layer.0.SelfAttention.o.weight || Tensor size: torch.Size([512, 384])\n",
            "Layer Name: encoder.block.1.layer.0.layer_norm.weight || Tensor size: torch.Size([512])\n",
            "Layer Name: encoder.block.1.layer.1.DenseReluDense.wi_0.weight || Tensor size: torch.Size([1024, 512])\n",
            "Layer Name: encoder.block.1.layer.1.DenseReluDense.wi_1.weight || Tensor size: torch.Size([1024, 512])\n",
            "Layer Name: encoder.block.1.layer.1.DenseReluDense.wo.weight || Tensor size: torch.Size([512, 1024])\n",
            "Layer Name: encoder.block.1.layer.1.layer_norm.weight || Tensor size: torch.Size([512])\n",
            "Layer Name: encoder.block.2.layer.0.SelfAttention.q.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: encoder.block.2.layer.0.SelfAttention.k.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: encoder.block.2.layer.0.SelfAttention.v.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: encoder.block.2.layer.0.SelfAttention.o.weight || Tensor size: torch.Size([512, 384])\n",
            "Layer Name: encoder.block.2.layer.0.layer_norm.weight || Tensor size: torch.Size([512])\n",
            "Layer Name: encoder.block.2.layer.1.DenseReluDense.wi_0.weight || Tensor size: torch.Size([1024, 512])\n",
            "Layer Name: encoder.block.2.layer.1.DenseReluDense.wi_1.weight || Tensor size: torch.Size([1024, 512])\n",
            "Layer Name: encoder.block.2.layer.1.DenseReluDense.wo.weight || Tensor size: torch.Size([512, 1024])\n",
            "Layer Name: encoder.block.2.layer.1.layer_norm.weight || Tensor size: torch.Size([512])\n",
            "Layer Name: encoder.block.3.layer.0.SelfAttention.q.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: encoder.block.3.layer.0.SelfAttention.k.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: encoder.block.3.layer.0.SelfAttention.v.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: encoder.block.3.layer.0.SelfAttention.o.weight || Tensor size: torch.Size([512, 384])\n",
            "Layer Name: encoder.block.3.layer.0.layer_norm.weight || Tensor size: torch.Size([512])\n",
            "Layer Name: encoder.block.3.layer.1.DenseReluDense.wi_0.weight || Tensor size: torch.Size([1024, 512])\n",
            "Layer Name: encoder.block.3.layer.1.DenseReluDense.wi_1.weight || Tensor size: torch.Size([1024, 512])\n",
            "Layer Name: encoder.block.3.layer.1.DenseReluDense.wo.weight || Tensor size: torch.Size([512, 1024])\n",
            "Layer Name: encoder.block.3.layer.1.layer_norm.weight || Tensor size: torch.Size([512])\n",
            "Layer Name: encoder.block.4.layer.0.SelfAttention.q.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: encoder.block.4.layer.0.SelfAttention.k.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: encoder.block.4.layer.0.SelfAttention.v.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: encoder.block.4.layer.0.SelfAttention.o.weight || Tensor size: torch.Size([512, 384])\n",
            "Layer Name: encoder.block.4.layer.0.layer_norm.weight || Tensor size: torch.Size([512])\n",
            "Layer Name: encoder.block.4.layer.1.DenseReluDense.wi_0.weight || Tensor size: torch.Size([1024, 512])\n",
            "Layer Name: encoder.block.4.layer.1.DenseReluDense.wi_1.weight || Tensor size: torch.Size([1024, 512])\n",
            "Layer Name: encoder.block.4.layer.1.DenseReluDense.wo.weight || Tensor size: torch.Size([512, 1024])\n",
            "Layer Name: encoder.block.4.layer.1.layer_norm.weight || Tensor size: torch.Size([512])\n",
            "Layer Name: encoder.block.5.layer.0.SelfAttention.q.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: encoder.block.5.layer.0.SelfAttention.k.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: encoder.block.5.layer.0.SelfAttention.v.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: encoder.block.5.layer.0.SelfAttention.o.weight || Tensor size: torch.Size([512, 384])\n",
            "Layer Name: encoder.block.5.layer.0.layer_norm.weight || Tensor size: torch.Size([512])\n",
            "Layer Name: encoder.block.5.layer.1.DenseReluDense.wi_0.weight || Tensor size: torch.Size([1024, 512])\n",
            "Layer Name: encoder.block.5.layer.1.DenseReluDense.wi_1.weight || Tensor size: torch.Size([1024, 512])\n",
            "Layer Name: encoder.block.5.layer.1.DenseReluDense.wo.weight || Tensor size: torch.Size([512, 1024])\n",
            "Layer Name: encoder.block.5.layer.1.layer_norm.weight || Tensor size: torch.Size([512])\n",
            "Layer Name: encoder.block.6.layer.0.SelfAttention.q.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: encoder.block.6.layer.0.SelfAttention.k.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: encoder.block.6.layer.0.SelfAttention.v.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: encoder.block.6.layer.0.SelfAttention.o.weight || Tensor size: torch.Size([512, 384])\n",
            "Layer Name: encoder.block.6.layer.0.layer_norm.weight || Tensor size: torch.Size([512])\n",
            "Layer Name: encoder.block.6.layer.1.DenseReluDense.wi_0.weight || Tensor size: torch.Size([1024, 512])\n",
            "Layer Name: encoder.block.6.layer.1.DenseReluDense.wi_1.weight || Tensor size: torch.Size([1024, 512])\n",
            "Layer Name: encoder.block.6.layer.1.DenseReluDense.wo.weight || Tensor size: torch.Size([512, 1024])\n",
            "Layer Name: encoder.block.6.layer.1.layer_norm.weight || Tensor size: torch.Size([512])\n",
            "Layer Name: encoder.block.7.layer.0.SelfAttention.q.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: encoder.block.7.layer.0.SelfAttention.k.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: encoder.block.7.layer.0.SelfAttention.v.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: encoder.block.7.layer.0.SelfAttention.o.weight || Tensor size: torch.Size([512, 384])\n",
            "Layer Name: encoder.block.7.layer.0.layer_norm.weight || Tensor size: torch.Size([512])\n",
            "Layer Name: encoder.block.7.layer.1.DenseReluDense.wi_0.weight || Tensor size: torch.Size([1024, 512])\n",
            "Layer Name: encoder.block.7.layer.1.DenseReluDense.wi_1.weight || Tensor size: torch.Size([1024, 512])\n",
            "Layer Name: encoder.block.7.layer.1.DenseReluDense.wo.weight || Tensor size: torch.Size([512, 1024])\n",
            "Layer Name: encoder.block.7.layer.1.layer_norm.weight || Tensor size: torch.Size([512])\n",
            "Layer Name: encoder.final_layer_norm.weight || Tensor size: torch.Size([512])\n",
            "Layer Name: decoder.block.0.layer.0.SelfAttention.q.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.0.layer.0.SelfAttention.k.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.0.layer.0.SelfAttention.v.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.0.layer.0.SelfAttention.o.weight || Tensor size: torch.Size([512, 384])\n",
            "Layer Name: decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight || Tensor size: torch.Size([32, 6])\n",
            "Layer Name: decoder.block.0.layer.0.layer_norm.weight || Tensor size: torch.Size([512])\n",
            "Layer Name: decoder.block.0.layer.1.EncDecAttention.q.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.0.layer.1.EncDecAttention.k.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.0.layer.1.EncDecAttention.v.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.0.layer.1.EncDecAttention.o.weight || Tensor size: torch.Size([512, 384])\n",
            "Layer Name: decoder.block.0.layer.1.layer_norm.weight || Tensor size: torch.Size([512])\n",
            "Layer Name: decoder.block.0.layer.2.DenseReluDense.wi_0.weight || Tensor size: torch.Size([1024, 512])\n",
            "Layer Name: decoder.block.0.layer.2.DenseReluDense.wi_1.weight || Tensor size: torch.Size([1024, 512])\n",
            "Layer Name: decoder.block.0.layer.2.DenseReluDense.wo.weight || Tensor size: torch.Size([512, 1024])\n",
            "Layer Name: decoder.block.0.layer.2.layer_norm.weight || Tensor size: torch.Size([512])\n",
            "Layer Name: decoder.block.1.layer.0.SelfAttention.q.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.1.layer.0.SelfAttention.k.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.1.layer.0.SelfAttention.v.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.1.layer.0.SelfAttention.o.weight || Tensor size: torch.Size([512, 384])\n",
            "Layer Name: decoder.block.1.layer.0.layer_norm.weight || Tensor size: torch.Size([512])\n",
            "Layer Name: decoder.block.1.layer.1.EncDecAttention.q.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.1.layer.1.EncDecAttention.k.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.1.layer.1.EncDecAttention.v.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.1.layer.1.EncDecAttention.o.weight || Tensor size: torch.Size([512, 384])\n",
            "Layer Name: decoder.block.1.layer.1.layer_norm.weight || Tensor size: torch.Size([512])\n",
            "Layer Name: decoder.block.1.layer.2.DenseReluDense.wi_0.weight || Tensor size: torch.Size([1024, 512])\n",
            "Layer Name: decoder.block.1.layer.2.DenseReluDense.wi_1.weight || Tensor size: torch.Size([1024, 512])\n",
            "Layer Name: decoder.block.1.layer.2.DenseReluDense.wo.weight || Tensor size: torch.Size([512, 1024])\n",
            "Layer Name: decoder.block.1.layer.2.layer_norm.weight || Tensor size: torch.Size([512])\n",
            "Layer Name: decoder.block.2.layer.0.SelfAttention.q.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.2.layer.0.SelfAttention.k.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.2.layer.0.SelfAttention.v.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.2.layer.0.SelfAttention.o.weight || Tensor size: torch.Size([512, 384])\n",
            "Layer Name: decoder.block.2.layer.0.layer_norm.weight || Tensor size: torch.Size([512])\n",
            "Layer Name: decoder.block.2.layer.1.EncDecAttention.q.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.2.layer.1.EncDecAttention.k.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.2.layer.1.EncDecAttention.v.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.2.layer.1.EncDecAttention.o.weight || Tensor size: torch.Size([512, 384])\n",
            "Layer Name: decoder.block.2.layer.1.layer_norm.weight || Tensor size: torch.Size([512])\n",
            "Layer Name: decoder.block.2.layer.2.DenseReluDense.wi_0.weight || Tensor size: torch.Size([1024, 512])\n",
            "Layer Name: decoder.block.2.layer.2.DenseReluDense.wi_1.weight || Tensor size: torch.Size([1024, 512])\n",
            "Layer Name: decoder.block.2.layer.2.DenseReluDense.wo.weight || Tensor size: torch.Size([512, 1024])\n",
            "Layer Name: decoder.block.2.layer.2.layer_norm.weight || Tensor size: torch.Size([512])\n",
            "Layer Name: decoder.block.3.layer.0.SelfAttention.q.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.3.layer.0.SelfAttention.k.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.3.layer.0.SelfAttention.v.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.3.layer.0.SelfAttention.o.weight || Tensor size: torch.Size([512, 384])\n",
            "Layer Name: decoder.block.3.layer.0.layer_norm.weight || Tensor size: torch.Size([512])\n",
            "Layer Name: decoder.block.3.layer.1.EncDecAttention.q.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.3.layer.1.EncDecAttention.k.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.3.layer.1.EncDecAttention.v.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.3.layer.1.EncDecAttention.o.weight || Tensor size: torch.Size([512, 384])\n",
            "Layer Name: decoder.block.3.layer.1.layer_norm.weight || Tensor size: torch.Size([512])\n",
            "Layer Name: decoder.block.3.layer.2.DenseReluDense.wi_0.weight || Tensor size: torch.Size([1024, 512])\n",
            "Layer Name: decoder.block.3.layer.2.DenseReluDense.wi_1.weight || Tensor size: torch.Size([1024, 512])\n",
            "Layer Name: decoder.block.3.layer.2.DenseReluDense.wo.weight || Tensor size: torch.Size([512, 1024])\n",
            "Layer Name: decoder.block.3.layer.2.layer_norm.weight || Tensor size: torch.Size([512])\n",
            "Layer Name: decoder.block.4.layer.0.SelfAttention.q.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.4.layer.0.SelfAttention.k.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.4.layer.0.SelfAttention.v.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.4.layer.0.SelfAttention.o.weight || Tensor size: torch.Size([512, 384])\n",
            "Layer Name: decoder.block.4.layer.0.layer_norm.weight || Tensor size: torch.Size([512])\n",
            "Layer Name: decoder.block.4.layer.1.EncDecAttention.q.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.4.layer.1.EncDecAttention.k.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.4.layer.1.EncDecAttention.v.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.4.layer.1.EncDecAttention.o.weight || Tensor size: torch.Size([512, 384])\n",
            "Layer Name: decoder.block.4.layer.1.layer_norm.weight || Tensor size: torch.Size([512])\n",
            "Layer Name: decoder.block.4.layer.2.DenseReluDense.wi_0.weight || Tensor size: torch.Size([1024, 512])\n",
            "Layer Name: decoder.block.4.layer.2.DenseReluDense.wi_1.weight || Tensor size: torch.Size([1024, 512])\n",
            "Layer Name: decoder.block.4.layer.2.DenseReluDense.wo.weight || Tensor size: torch.Size([512, 1024])\n",
            "Layer Name: decoder.block.4.layer.2.layer_norm.weight || Tensor size: torch.Size([512])\n",
            "Layer Name: decoder.block.5.layer.0.SelfAttention.q.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.5.layer.0.SelfAttention.k.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.5.layer.0.SelfAttention.v.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.5.layer.0.SelfAttention.o.weight || Tensor size: torch.Size([512, 384])\n",
            "Layer Name: decoder.block.5.layer.0.layer_norm.weight || Tensor size: torch.Size([512])\n",
            "Layer Name: decoder.block.5.layer.1.EncDecAttention.q.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.5.layer.1.EncDecAttention.k.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.5.layer.1.EncDecAttention.v.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.5.layer.1.EncDecAttention.o.weight || Tensor size: torch.Size([512, 384])\n",
            "Layer Name: decoder.block.5.layer.1.layer_norm.weight || Tensor size: torch.Size([512])\n",
            "Layer Name: decoder.block.5.layer.2.DenseReluDense.wi_0.weight || Tensor size: torch.Size([1024, 512])\n",
            "Layer Name: decoder.block.5.layer.2.DenseReluDense.wi_1.weight || Tensor size: torch.Size([1024, 512])\n",
            "Layer Name: decoder.block.5.layer.2.DenseReluDense.wo.weight || Tensor size: torch.Size([512, 1024])\n",
            "Layer Name: decoder.block.5.layer.2.layer_norm.weight || Tensor size: torch.Size([512])\n",
            "Layer Name: decoder.block.6.layer.0.SelfAttention.q.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.6.layer.0.SelfAttention.k.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.6.layer.0.SelfAttention.v.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.6.layer.0.SelfAttention.o.weight || Tensor size: torch.Size([512, 384])\n",
            "Layer Name: decoder.block.6.layer.0.layer_norm.weight || Tensor size: torch.Size([512])\n",
            "Layer Name: decoder.block.6.layer.1.EncDecAttention.q.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.6.layer.1.EncDecAttention.k.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.6.layer.1.EncDecAttention.v.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.6.layer.1.EncDecAttention.o.weight || Tensor size: torch.Size([512, 384])\n",
            "Layer Name: decoder.block.6.layer.1.layer_norm.weight || Tensor size: torch.Size([512])\n",
            "Layer Name: decoder.block.6.layer.2.DenseReluDense.wi_0.weight || Tensor size: torch.Size([1024, 512])\n",
            "Layer Name: decoder.block.6.layer.2.DenseReluDense.wi_1.weight || Tensor size: torch.Size([1024, 512])\n",
            "Layer Name: decoder.block.6.layer.2.DenseReluDense.wo.weight || Tensor size: torch.Size([512, 1024])\n",
            "Layer Name: decoder.block.6.layer.2.layer_norm.weight || Tensor size: torch.Size([512])\n",
            "Layer Name: decoder.block.7.layer.0.SelfAttention.q.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.7.layer.0.SelfAttention.k.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.7.layer.0.SelfAttention.v.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.7.layer.0.SelfAttention.o.weight || Tensor size: torch.Size([512, 384])\n",
            "Layer Name: decoder.block.7.layer.0.layer_norm.weight || Tensor size: torch.Size([512])\n",
            "Layer Name: decoder.block.7.layer.1.EncDecAttention.q.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.7.layer.1.EncDecAttention.k.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.7.layer.1.EncDecAttention.v.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.7.layer.1.EncDecAttention.o.weight || Tensor size: torch.Size([512, 384])\n",
            "Layer Name: decoder.block.7.layer.1.layer_norm.weight || Tensor size: torch.Size([512])\n",
            "Layer Name: decoder.block.7.layer.2.DenseReluDense.wi_0.weight || Tensor size: torch.Size([1024, 512])\n",
            "Layer Name: decoder.block.7.layer.2.DenseReluDense.wi_1.weight || Tensor size: torch.Size([1024, 512])\n",
            "Layer Name: decoder.block.7.layer.2.DenseReluDense.wo.weight || Tensor size: torch.Size([512, 1024])\n",
            "Layer Name: decoder.block.7.layer.2.layer_norm.weight || Tensor size: torch.Size([512])\n",
            "Layer Name: decoder.final_layer_norm.weight || Tensor size: torch.Size([512])\n",
            "Layer Name: lm_head.weight || Tensor size: torch.Size([32128, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Total trainable parameters: {total_params:_}, Size in MB: {total_size/(1024*1024)}MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vb6sY1ExMpju",
        "outputId": "c6fd269c-7170-414e-85fc-f1c0222f51c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total trainable parameters: 76_961_152, Size in MB: 293.58349609375MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting tensor in model.decoder.final_layer_norm.weight to 0.\n"
      ],
      "metadata": {
        "id": "JR4CT4kAOzer"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  model.decoder.final_layer_norm.weight = torch.nn.Parameter(torch.zeros_like(model.decoder.final_layer_norm.weight))"
      ],
      "metadata": {
        "id": "Z7GOo5xENNCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the altered layer:"
      ],
      "metadata": {
        "id": "YjXRef2eRpJs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.decoder.final_layer_norm.weight"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZ3kNsB7PFNU",
        "outputId": "48dcb6a0-bd83-45d7-9f91-ec539e6e91ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "translation after weight resetting"
      ],
      "metadata": {
        "id": "STv0d7lRKNgj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(\"translate from English to French: A step by step recipe to make bolognese pasta\", return_tensors=\"pt\")\n",
        "outputs = model.generate(**inputs, num_beams = 4, do_sample = True)\n",
        "print(tokenizer.batch_decode(outputs, skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJVJspU7Rmm_",
        "outputId": "5f102c04-b0a3-4a1c-f66a-bd5e00e618ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['MessengerDu registered subsidiaryalternating cellesnnouncing dual enclosure needed Divi duration rămas Zauberych categoriestorilor retailers Füße']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q&A after weight resetting"
      ],
      "metadata": {
        "id": "WEJgZNqUKQhf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(\"\"\"Answer the following question based on the context.\\nContext: Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\\nQuestion: To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\"\"\", return_tensors=\"pt\")\n",
        "outputs = model.generate(**inputs, num_beams = 4, do_sample = True)\n",
        "print(tokenizer.batch_decode(outputs, skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sz43hdWWKMdY",
        "outputId": "fd976730-c596-49d0-e5e7-1aa5e7d53bee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['aß mecanic Lieb acuz 16 Alicefab possession butcutaneousoléfindéanmoins jumpscheibe Bat Irving cadouri marks']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Replacing layers with smaller dimensions"
      ],
      "metadata": {
        "id": "FQfFpik97f7D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  model.decoder.final_layer_norm.weight = torch.nn.Parameter(torch.randn(384))\n",
        "  model.decoder.block[7].layer[2].layer_norm.weight = torch.nn.Parameter(torch.randn(384))\n",
        "  model.decoder.block[7].layer[2].DenseReluDense.wo.weight = torch.nn.Parameter(torch.randn(384,1024))\n",
        "  model.lm_head.weight = torch.nn.Parameter(torch.randn(384, 32128))"
      ],
      "metadata": {
        "id": "BtZozkbgX38s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = 0\n",
        "total_size = 0\n",
        "for name,param in model.named_parameters():\n",
        "  print(f\"Layer Name: {name} || Tensor size: {param.size()}\")\n",
        "  total_params += param.numel()\n",
        "  total_size += param.numel() * param.element_size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4FJ0WFqq-Vfi",
        "outputId": "0acb0a81-c9f1-4acd-b0d0-22455e668f9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer Name: shared.weight || Tensor size: torch.Size([32128, 512])\n",
            "Layer Name: encoder.block.0.layer.0.SelfAttention.q.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: encoder.block.0.layer.0.SelfAttention.k.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: encoder.block.0.layer.0.SelfAttention.v.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: encoder.block.0.layer.0.SelfAttention.o.weight || Tensor size: torch.Size([512, 384])\n",
            "Layer Name: encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight || Tensor size: torch.Size([32, 6])\n",
            "Layer Name: encoder.block.0.layer.0.layer_norm.weight || Tensor size: torch.Size([512])\n",
            "Layer Name: encoder.block.0.layer.1.DenseReluDense.wi_0.weight || Tensor size: torch.Size([1024, 512])\n",
            "Layer Name: encoder.block.0.layer.1.DenseReluDense.wi_1.weight || Tensor size: torch.Size([1024, 512])\n",
            "Layer Name: encoder.block.0.layer.1.DenseReluDense.wo.weight || Tensor size: torch.Size([512, 1024])\n",
            "Layer Name: encoder.block.0.layer.1.layer_norm.weight || Tensor size: torch.Size([512])\n",
            "Layer Name: encoder.block.1.layer.0.SelfAttention.q.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: encoder.block.1.layer.0.SelfAttention.k.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: encoder.block.1.layer.0.SelfAttention.v.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: encoder.block.1.layer.0.SelfAttention.o.weight || Tensor size: torch.Size([512, 384])\n",
            "Layer Name: encoder.block.1.layer.0.layer_norm.weight || Tensor size: torch.Size([512])\n",
            "Layer Name: encoder.block.1.layer.1.DenseReluDense.wi_0.weight || Tensor size: torch.Size([1024, 512])\n",
            "Layer Name: encoder.block.1.layer.1.DenseReluDense.wi_1.weight || Tensor size: torch.Size([1024, 512])\n",
            "Layer Name: encoder.block.1.layer.1.DenseReluDense.wo.weight || Tensor size: torch.Size([512, 1024])\n",
            "Layer Name: encoder.block.1.layer.1.layer_norm.weight || Tensor size: torch.Size([512])\n",
            "Layer Name: encoder.block.2.layer.0.SelfAttention.q.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: encoder.block.2.layer.0.SelfAttention.k.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: encoder.block.2.layer.0.SelfAttention.v.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: encoder.block.2.layer.0.SelfAttention.o.weight || Tensor size: torch.Size([512, 384])\n",
            "Layer Name: encoder.block.2.layer.0.layer_norm.weight || Tensor size: torch.Size([512])\n",
            "Layer Name: encoder.block.2.layer.1.DenseReluDense.wi_0.weight || Tensor size: torch.Size([1024, 512])\n",
            "Layer Name: encoder.block.2.layer.1.DenseReluDense.wi_1.weight || Tensor size: torch.Size([1024, 512])\n",
            "Layer Name: encoder.block.2.layer.1.DenseReluDense.wo.weight || Tensor size: torch.Size([512, 1024])\n",
            "Layer Name: encoder.block.2.layer.1.layer_norm.weight || Tensor size: torch.Size([512])\n",
            "Layer Name: encoder.block.3.layer.0.SelfAttention.q.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: encoder.block.3.layer.0.SelfAttention.k.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: encoder.block.3.layer.0.SelfAttention.v.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: encoder.block.3.layer.0.SelfAttention.o.weight || Tensor size: torch.Size([512, 384])\n",
            "Layer Name: encoder.block.3.layer.0.layer_norm.weight || Tensor size: torch.Size([512])\n",
            "Layer Name: encoder.block.3.layer.1.DenseReluDense.wi_0.weight || Tensor size: torch.Size([1024, 512])\n",
            "Layer Name: encoder.block.3.layer.1.DenseReluDense.wi_1.weight || Tensor size: torch.Size([1024, 512])\n",
            "Layer Name: encoder.block.3.layer.1.DenseReluDense.wo.weight || Tensor size: torch.Size([512, 1024])\n",
            "Layer Name: encoder.block.3.layer.1.layer_norm.weight || Tensor size: torch.Size([512])\n",
            "Layer Name: encoder.block.4.layer.0.SelfAttention.q.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: encoder.block.4.layer.0.SelfAttention.k.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: encoder.block.4.layer.0.SelfAttention.v.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: encoder.block.4.layer.0.SelfAttention.o.weight || Tensor size: torch.Size([512, 384])\n",
            "Layer Name: encoder.block.4.layer.0.layer_norm.weight || Tensor size: torch.Size([512])\n",
            "Layer Name: encoder.block.4.layer.1.DenseReluDense.wi_0.weight || Tensor size: torch.Size([1024, 512])\n",
            "Layer Name: encoder.block.4.layer.1.DenseReluDense.wi_1.weight || Tensor size: torch.Size([1024, 512])\n",
            "Layer Name: encoder.block.4.layer.1.DenseReluDense.wo.weight || Tensor size: torch.Size([512, 1024])\n",
            "Layer Name: encoder.block.4.layer.1.layer_norm.weight || Tensor size: torch.Size([512])\n",
            "Layer Name: encoder.block.5.layer.0.SelfAttention.q.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: encoder.block.5.layer.0.SelfAttention.k.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: encoder.block.5.layer.0.SelfAttention.v.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: encoder.block.5.layer.0.SelfAttention.o.weight || Tensor size: torch.Size([512, 384])\n",
            "Layer Name: encoder.block.5.layer.0.layer_norm.weight || Tensor size: torch.Size([512])\n",
            "Layer Name: encoder.block.5.layer.1.DenseReluDense.wi_0.weight || Tensor size: torch.Size([1024, 512])\n",
            "Layer Name: encoder.block.5.layer.1.DenseReluDense.wi_1.weight || Tensor size: torch.Size([1024, 512])\n",
            "Layer Name: encoder.block.5.layer.1.DenseReluDense.wo.weight || Tensor size: torch.Size([512, 1024])\n",
            "Layer Name: encoder.block.5.layer.1.layer_norm.weight || Tensor size: torch.Size([512])\n",
            "Layer Name: encoder.block.6.layer.0.SelfAttention.q.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: encoder.block.6.layer.0.SelfAttention.k.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: encoder.block.6.layer.0.SelfAttention.v.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: encoder.block.6.layer.0.SelfAttention.o.weight || Tensor size: torch.Size([512, 384])\n",
            "Layer Name: encoder.block.6.layer.0.layer_norm.weight || Tensor size: torch.Size([512])\n",
            "Layer Name: encoder.block.6.layer.1.DenseReluDense.wi_0.weight || Tensor size: torch.Size([1024, 512])\n",
            "Layer Name: encoder.block.6.layer.1.DenseReluDense.wi_1.weight || Tensor size: torch.Size([1024, 512])\n",
            "Layer Name: encoder.block.6.layer.1.DenseReluDense.wo.weight || Tensor size: torch.Size([512, 1024])\n",
            "Layer Name: encoder.block.6.layer.1.layer_norm.weight || Tensor size: torch.Size([512])\n",
            "Layer Name: encoder.block.7.layer.0.SelfAttention.q.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: encoder.block.7.layer.0.SelfAttention.k.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: encoder.block.7.layer.0.SelfAttention.v.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: encoder.block.7.layer.0.SelfAttention.o.weight || Tensor size: torch.Size([512, 384])\n",
            "Layer Name: encoder.block.7.layer.0.layer_norm.weight || Tensor size: torch.Size([512])\n",
            "Layer Name: encoder.block.7.layer.1.DenseReluDense.wi_0.weight || Tensor size: torch.Size([1024, 512])\n",
            "Layer Name: encoder.block.7.layer.1.DenseReluDense.wi_1.weight || Tensor size: torch.Size([1024, 512])\n",
            "Layer Name: encoder.block.7.layer.1.DenseReluDense.wo.weight || Tensor size: torch.Size([512, 1024])\n",
            "Layer Name: encoder.block.7.layer.1.layer_norm.weight || Tensor size: torch.Size([512])\n",
            "Layer Name: encoder.final_layer_norm.weight || Tensor size: torch.Size([512])\n",
            "Layer Name: decoder.block.0.layer.0.SelfAttention.q.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.0.layer.0.SelfAttention.k.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.0.layer.0.SelfAttention.v.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.0.layer.0.SelfAttention.o.weight || Tensor size: torch.Size([512, 384])\n",
            "Layer Name: decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight || Tensor size: torch.Size([32, 6])\n",
            "Layer Name: decoder.block.0.layer.0.layer_norm.weight || Tensor size: torch.Size([512])\n",
            "Layer Name: decoder.block.0.layer.1.EncDecAttention.q.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.0.layer.1.EncDecAttention.k.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.0.layer.1.EncDecAttention.v.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.0.layer.1.EncDecAttention.o.weight || Tensor size: torch.Size([512, 384])\n",
            "Layer Name: decoder.block.0.layer.1.layer_norm.weight || Tensor size: torch.Size([512])\n",
            "Layer Name: decoder.block.0.layer.2.DenseReluDense.wi_0.weight || Tensor size: torch.Size([1024, 512])\n",
            "Layer Name: decoder.block.0.layer.2.DenseReluDense.wi_1.weight || Tensor size: torch.Size([1024, 512])\n",
            "Layer Name: decoder.block.0.layer.2.DenseReluDense.wo.weight || Tensor size: torch.Size([512, 1024])\n",
            "Layer Name: decoder.block.0.layer.2.layer_norm.weight || Tensor size: torch.Size([512])\n",
            "Layer Name: decoder.block.1.layer.0.SelfAttention.q.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.1.layer.0.SelfAttention.k.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.1.layer.0.SelfAttention.v.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.1.layer.0.SelfAttention.o.weight || Tensor size: torch.Size([512, 384])\n",
            "Layer Name: decoder.block.1.layer.0.layer_norm.weight || Tensor size: torch.Size([512])\n",
            "Layer Name: decoder.block.1.layer.1.EncDecAttention.q.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.1.layer.1.EncDecAttention.k.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.1.layer.1.EncDecAttention.v.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.1.layer.1.EncDecAttention.o.weight || Tensor size: torch.Size([512, 384])\n",
            "Layer Name: decoder.block.1.layer.1.layer_norm.weight || Tensor size: torch.Size([512])\n",
            "Layer Name: decoder.block.1.layer.2.DenseReluDense.wi_0.weight || Tensor size: torch.Size([1024, 512])\n",
            "Layer Name: decoder.block.1.layer.2.DenseReluDense.wi_1.weight || Tensor size: torch.Size([1024, 512])\n",
            "Layer Name: decoder.block.1.layer.2.DenseReluDense.wo.weight || Tensor size: torch.Size([512, 1024])\n",
            "Layer Name: decoder.block.1.layer.2.layer_norm.weight || Tensor size: torch.Size([512])\n",
            "Layer Name: decoder.block.2.layer.0.SelfAttention.q.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.2.layer.0.SelfAttention.k.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.2.layer.0.SelfAttention.v.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.2.layer.0.SelfAttention.o.weight || Tensor size: torch.Size([512, 384])\n",
            "Layer Name: decoder.block.2.layer.0.layer_norm.weight || Tensor size: torch.Size([512])\n",
            "Layer Name: decoder.block.2.layer.1.EncDecAttention.q.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.2.layer.1.EncDecAttention.k.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.2.layer.1.EncDecAttention.v.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.2.layer.1.EncDecAttention.o.weight || Tensor size: torch.Size([512, 384])\n",
            "Layer Name: decoder.block.2.layer.1.layer_norm.weight || Tensor size: torch.Size([512])\n",
            "Layer Name: decoder.block.2.layer.2.DenseReluDense.wi_0.weight || Tensor size: torch.Size([1024, 512])\n",
            "Layer Name: decoder.block.2.layer.2.DenseReluDense.wi_1.weight || Tensor size: torch.Size([1024, 512])\n",
            "Layer Name: decoder.block.2.layer.2.DenseReluDense.wo.weight || Tensor size: torch.Size([512, 1024])\n",
            "Layer Name: decoder.block.2.layer.2.layer_norm.weight || Tensor size: torch.Size([512])\n",
            "Layer Name: decoder.block.3.layer.0.SelfAttention.q.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.3.layer.0.SelfAttention.k.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.3.layer.0.SelfAttention.v.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.3.layer.0.SelfAttention.o.weight || Tensor size: torch.Size([512, 384])\n",
            "Layer Name: decoder.block.3.layer.0.layer_norm.weight || Tensor size: torch.Size([512])\n",
            "Layer Name: decoder.block.3.layer.1.EncDecAttention.q.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.3.layer.1.EncDecAttention.k.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.3.layer.1.EncDecAttention.v.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.3.layer.1.EncDecAttention.o.weight || Tensor size: torch.Size([512, 384])\n",
            "Layer Name: decoder.block.3.layer.1.layer_norm.weight || Tensor size: torch.Size([512])\n",
            "Layer Name: decoder.block.3.layer.2.DenseReluDense.wi_0.weight || Tensor size: torch.Size([1024, 512])\n",
            "Layer Name: decoder.block.3.layer.2.DenseReluDense.wi_1.weight || Tensor size: torch.Size([1024, 512])\n",
            "Layer Name: decoder.block.3.layer.2.DenseReluDense.wo.weight || Tensor size: torch.Size([512, 1024])\n",
            "Layer Name: decoder.block.3.layer.2.layer_norm.weight || Tensor size: torch.Size([512])\n",
            "Layer Name: decoder.block.4.layer.0.SelfAttention.q.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.4.layer.0.SelfAttention.k.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.4.layer.0.SelfAttention.v.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.4.layer.0.SelfAttention.o.weight || Tensor size: torch.Size([512, 384])\n",
            "Layer Name: decoder.block.4.layer.0.layer_norm.weight || Tensor size: torch.Size([512])\n",
            "Layer Name: decoder.block.4.layer.1.EncDecAttention.q.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.4.layer.1.EncDecAttention.k.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.4.layer.1.EncDecAttention.v.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.4.layer.1.EncDecAttention.o.weight || Tensor size: torch.Size([512, 384])\n",
            "Layer Name: decoder.block.4.layer.1.layer_norm.weight || Tensor size: torch.Size([512])\n",
            "Layer Name: decoder.block.4.layer.2.DenseReluDense.wi_0.weight || Tensor size: torch.Size([1024, 512])\n",
            "Layer Name: decoder.block.4.layer.2.DenseReluDense.wi_1.weight || Tensor size: torch.Size([1024, 512])\n",
            "Layer Name: decoder.block.4.layer.2.DenseReluDense.wo.weight || Tensor size: torch.Size([512, 1024])\n",
            "Layer Name: decoder.block.4.layer.2.layer_norm.weight || Tensor size: torch.Size([512])\n",
            "Layer Name: decoder.block.5.layer.0.SelfAttention.q.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.5.layer.0.SelfAttention.k.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.5.layer.0.SelfAttention.v.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.5.layer.0.SelfAttention.o.weight || Tensor size: torch.Size([512, 384])\n",
            "Layer Name: decoder.block.5.layer.0.layer_norm.weight || Tensor size: torch.Size([512])\n",
            "Layer Name: decoder.block.5.layer.1.EncDecAttention.q.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.5.layer.1.EncDecAttention.k.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.5.layer.1.EncDecAttention.v.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.5.layer.1.EncDecAttention.o.weight || Tensor size: torch.Size([512, 384])\n",
            "Layer Name: decoder.block.5.layer.1.layer_norm.weight || Tensor size: torch.Size([512])\n",
            "Layer Name: decoder.block.5.layer.2.DenseReluDense.wi_0.weight || Tensor size: torch.Size([1024, 512])\n",
            "Layer Name: decoder.block.5.layer.2.DenseReluDense.wi_1.weight || Tensor size: torch.Size([1024, 512])\n",
            "Layer Name: decoder.block.5.layer.2.DenseReluDense.wo.weight || Tensor size: torch.Size([512, 1024])\n",
            "Layer Name: decoder.block.5.layer.2.layer_norm.weight || Tensor size: torch.Size([512])\n",
            "Layer Name: decoder.block.6.layer.0.SelfAttention.q.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.6.layer.0.SelfAttention.k.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.6.layer.0.SelfAttention.v.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.6.layer.0.SelfAttention.o.weight || Tensor size: torch.Size([512, 384])\n",
            "Layer Name: decoder.block.6.layer.0.layer_norm.weight || Tensor size: torch.Size([512])\n",
            "Layer Name: decoder.block.6.layer.1.EncDecAttention.q.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.6.layer.1.EncDecAttention.k.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.6.layer.1.EncDecAttention.v.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.6.layer.1.EncDecAttention.o.weight || Tensor size: torch.Size([512, 384])\n",
            "Layer Name: decoder.block.6.layer.1.layer_norm.weight || Tensor size: torch.Size([512])\n",
            "Layer Name: decoder.block.6.layer.2.DenseReluDense.wi_0.weight || Tensor size: torch.Size([1024, 512])\n",
            "Layer Name: decoder.block.6.layer.2.DenseReluDense.wi_1.weight || Tensor size: torch.Size([1024, 512])\n",
            "Layer Name: decoder.block.6.layer.2.DenseReluDense.wo.weight || Tensor size: torch.Size([512, 1024])\n",
            "Layer Name: decoder.block.6.layer.2.layer_norm.weight || Tensor size: torch.Size([512])\n",
            "Layer Name: decoder.block.7.layer.0.SelfAttention.q.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.7.layer.0.SelfAttention.k.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.7.layer.0.SelfAttention.v.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.7.layer.0.SelfAttention.o.weight || Tensor size: torch.Size([512, 384])\n",
            "Layer Name: decoder.block.7.layer.0.layer_norm.weight || Tensor size: torch.Size([512])\n",
            "Layer Name: decoder.block.7.layer.1.EncDecAttention.q.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.7.layer.1.EncDecAttention.k.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.7.layer.1.EncDecAttention.v.weight || Tensor size: torch.Size([384, 512])\n",
            "Layer Name: decoder.block.7.layer.1.EncDecAttention.o.weight || Tensor size: torch.Size([512, 384])\n",
            "Layer Name: decoder.block.7.layer.1.layer_norm.weight || Tensor size: torch.Size([512])\n",
            "Layer Name: decoder.block.7.layer.2.DenseReluDense.wi_0.weight || Tensor size: torch.Size([1024, 512])\n",
            "Layer Name: decoder.block.7.layer.2.DenseReluDense.wi_1.weight || Tensor size: torch.Size([1024, 512])\n",
            "Layer Name: decoder.block.7.layer.2.DenseReluDense.wo.weight || Tensor size: torch.Size([384, 1024])\n",
            "Layer Name: decoder.block.7.layer.2.layer_norm.weight || Tensor size: torch.Size([384])\n",
            "Layer Name: decoder.final_layer_norm.weight || Tensor size: torch.Size([384])\n",
            "Layer Name: lm_head.weight || Tensor size: torch.Size([384, 32128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PART-2 finetuning the flan-t5 model on SQuAD dataset\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "HjaqZ3NsXOcf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "Rb5YDd1DgV1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "train_dataset = load_dataset('squad', split='train')\n",
        "val_dataset = load_dataset('squad', split='validation')"
      ],
      "metadata": {
        "id": "VuO436mBZBg6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7FZZaa9gOFF",
        "outputId": "466d2f7b-6587-4b9c-8c9b-22db476509bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "    num_rows: 87599\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xeXFFbqvBCJb",
        "outputId": "a662d452-eadf-4c94-dc2a-cc66b945b8a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "    num_rows: 10570\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset.features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdhWr1-_gpEZ",
        "outputId": "1b7999c1-04a7-4368-be7a-f684138391ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': Value(dtype='string', id=None),\n",
              " 'title': Value(dtype='string', id=None),\n",
              " 'context': Value(dtype='string', id=None),\n",
              " 'question': Value(dtype='string', id=None),\n",
              " 'answers': Sequence(feature={'text': Value(dtype='string', id=None), 'answer_start': Value(dtype='int32', id=None)}, length=-1, id=None)}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ogmQn1lgyRb",
        "outputId": "2f91d9d0-e037-4705-8a16-37861d903adb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': '5733be284776f41900661182',\n",
              " 'title': 'University_of_Notre_Dame',\n",
              " 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n",
              " 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?',\n",
              " 'answers': {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CONTEXT, QUESTION, ANSWER = train_dataset['context'], train_dataset['question'], train_dataset['answers']\n",
        "VAL_CONTEXT, VAL_QUESTION, VAL_ANSWER = val_dataset['context'], val_dataset['question'], val_dataset['answers']"
      ],
      "metadata": {
        "id": "PisOc3FNg0VS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Distribution of answer lenghts in the dataset (#space_seperated_tokens)"
      ],
      "metadata": {
        "id": "j9dGC3s-Mjtw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot([len(s['text'][0].split()) for s in ANSWER])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "c0RUhLhaEo2r",
        "outputId": "9132e0a1-4dd8-4713-c1f3-f13d3bea5669"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7df703a99750>]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(CONTEXT) == len(QUESTION) ==  len(ANSWER)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIIqK1ZPngU3",
        "outputId": "3e180f4a-22cd-4249-8ca9-394c70069993"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")"
      ],
      "metadata": {
        "id": "gYimFBDL7VVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from typing import List, Dict\n",
        "import random\n",
        "\n",
        "class SquadDataset(Dataset):\n",
        "  def __init__(self, CONTEXT: List[str], QUESTION: List[str], ANSWER: List[dict[str: List[int|str]]]):\n",
        "\n",
        "    self.CONTEXT, self.QUESTION = CONTEXT, QUESTION\n",
        "\n",
        "    self.ANSWER = [items['text'][0] for items in ANSWER]\n",
        "\n",
        "    assert len(self.CONTEXT) == len(self.QUESTION) ==  len(self.ANSWER)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.CONTEXT)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "\n",
        "    # sample one instruction form the given set\n",
        "    prefix = random.sample(['Give short one-liner answer for the question based on the context.', 'Read the given context and answer the following question.'], 1)\n",
        "\n",
        "    return {\n",
        "        \"encoder_input\": f\"\"\"{prefix}\\nQuestion: {self.QUESTION[index]}\\nContext: {self.CONTEXT[index]}\"\"\",\n",
        "        \"output\": self.ANSWER[index]\n",
        "    }\n",
        "\n",
        "\n",
        "dataset_size = len(train_dataset)\n",
        "\n",
        "# setting the train and validation sizes to smaller values\n",
        "TRAIN_SIZE = 40000\n",
        "VAL_SIZE = 5000\n",
        "\n",
        "TrainDataset = SquadDataset(CONTEXT[:TRAIN_SIZE], QUESTION[:TRAIN_SIZE], ANSWER[:TRAIN_SIZE])\n",
        "ValDataset = SquadDataset(VAL_CONTEXT[:VAL_SIZE], VAL_QUESTION[:VAL_SIZE], VAL_ANSWER[:VAL_SIZE])\n",
        "\n",
        "BATCH_SIZE = 5\n",
        "train_dataloader = DataLoader(TrainDataset, shuffle = True, batch_size = BATCH_SIZE)\n",
        "val_dataloader = DataLoader(ValDataset, shuffle = False, batch_size = BATCH_SIZE)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "YtaSOCY6mqy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def QA_generation(model, data_loader):\n",
        "  GroundTruth_answers = []\n",
        "  Predicted_answers = []\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "    for step,X in enumerate(data_loader):\n",
        "\n",
        "        inputs = tokenizer(X['encoder_input'], padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
        "        outputs = model.generate(**inputs, num_beams=4,\n",
        "                                do_sample=True,\n",
        "                                min_length=1,\n",
        "                                max_length=30,\n",
        "                                eos_token_id=model.config.eos_token_id\n",
        "                                )\n",
        "        outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "        GroundTruth_answers.extend(X['output'])\n",
        "        Predicted_answers.extend(outputs)\n",
        "\n",
        "  return GroundTruth_answers, Predicted_answers"
      ],
      "metadata": {
        "id": "1zrgJEY91jRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GroundTruth_answers, pretrained_Predicted_answers = QA_generation(model, val_dataloader)"
      ],
      "metadata": {
        "id": "YzLGzwbw13e0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import Adam\n",
        "\n",
        "optimizer = Adam(model.parameters(), lr = 3e-4)"
      ],
      "metadata": {
        "id": "Q3QK7CJM0RKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dPhASk6shZE",
        "outputId": "421b631d-1562-4fe5-b96d-6655020de05e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm\n",
        "\n",
        "EPOCHS = 1\n",
        "LOSS = []\n",
        "VAL_LOSS = []\n",
        "train_steps = 0\n",
        "loss = 0\n",
        "total_loss = 0\n",
        "gradient_accumulation_steps = 10 # effective batch_size = 40\n",
        "validation_steps = 100\n",
        "\n",
        "train_loss = 0\n",
        "for n_epoch in tqdm.tqdm(range(EPOCHS)):\n",
        "  model.train()\n",
        "\n",
        "  for step,X in enumerate(train_dataloader):\n",
        "    train_steps += 1\n",
        "\n",
        "    inputs = tokenizer(X['encoder_input'], padding=True, truncation=True, return_tensors = 'pt').to(device)\n",
        "    output = tokenizer(text_target=X['output'] , padding=True, truncation=True, return_tensors = 'pt').to(device)\n",
        "\n",
        "    loss = model(**inputs, labels=output['input_ids']).loss\n",
        "\n",
        "    loss.backward()\n",
        "    total_loss += loss.item()\n",
        "\n",
        "    if train_steps%gradient_accumulation_steps == 0:\n",
        "\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      LOSS.append(total_loss/(gradient_accumulation_steps * BATCH_SIZE))\n",
        "      torch.cuda.empty_cache()\n",
        "\n",
        "      train_loss += total_loss\n",
        "      total_loss = 0\n",
        "\n",
        "    if train_steps%validation_steps == 0:\n",
        "\n",
        "      model.eval()\n",
        "      val_loss = 0\n",
        "\n",
        "      with torch.no_grad():\n",
        "        for step,Y in enumerate(val_dataloader):\n",
        "\n",
        "          inputs = tokenizer(Y['encoder_input'], padding=True, truncation=True, return_tensors = 'pt').to(device)\n",
        "          output = tokenizer(text_target=Y['output'] , padding=True, truncation=True, return_tensors = 'pt').to(device)\n",
        "\n",
        "          loss = model(**inputs, labels=output['input_ids']).loss\n",
        "          val_loss += loss.item()\n",
        "\n",
        "      VAL_LOSS.append(val_loss/(len(val_dataloader)*BATCH_SIZE))\n",
        "\n",
        "      print(f\"Train Steps: {train_steps}, Avg. Training Loss: {train_loss/(validation_steps*BATCH_SIZE):0.5f}, Avg. Validation Loss: {val_loss/(len(val_dataloader)*BATCH_SIZE):.5f}\")\n",
        "\n",
        "      train_loss = 0\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GWM2uudsDqe",
        "outputId": "9b4448cd-a848-474c-d020-af45d7527ad4"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Steps: 100, Avg. Training Loss: 2.91862, Avg. Validation Loss: 1.69251\n",
            "Train Steps: 200, Avg. Training Loss: 1.26650, Avg. Validation Loss: 0.86743\n",
            "Train Steps: 300, Avg. Training Loss: 0.65730, Avg. Validation Loss: 0.48821\n",
            "Train Steps: 400, Avg. Training Loss: 0.42854, Avg. Validation Loss: 0.35133\n",
            "Train Steps: 500, Avg. Training Loss: 0.29941, Avg. Validation Loss: 0.25264\n",
            "Train Steps: 600, Avg. Training Loss: 0.19086, Avg. Validation Loss: 0.17444\n",
            "Train Steps: 700, Avg. Training Loss: 0.13654, Avg. Validation Loss: 0.13007\n",
            "Train Steps: 800, Avg. Training Loss: 0.09405, Avg. Validation Loss: 0.11277\n",
            "Train Steps: 900, Avg. Training Loss: 0.07561, Avg. Validation Loss: 0.10552\n",
            "Train Steps: 1000, Avg. Training Loss: 0.07319, Avg. Validation Loss: 0.10044\n",
            "Train Steps: 1100, Avg. Training Loss: 0.08453, Avg. Validation Loss: 0.09672\n",
            "Train Steps: 1200, Avg. Training Loss: 0.06221, Avg. Validation Loss: 0.09478\n",
            "Train Steps: 1300, Avg. Training Loss: 0.06714, Avg. Validation Loss: 0.09238\n",
            "Train Steps: 1400, Avg. Training Loss: 0.06657, Avg. Validation Loss: 0.09391\n",
            "Train Steps: 1500, Avg. Training Loss: 0.06024, Avg. Validation Loss: 0.09230\n",
            "Train Steps: 1600, Avg. Training Loss: 0.06865, Avg. Validation Loss: 0.08912\n",
            "Train Steps: 1700, Avg. Training Loss: 0.06742, Avg. Validation Loss: 0.09091\n",
            "Train Steps: 1800, Avg. Training Loss: 0.06672, Avg. Validation Loss: 0.08848\n",
            "Train Steps: 1900, Avg. Training Loss: 0.07205, Avg. Validation Loss: 0.08848\n",
            "Train Steps: 2000, Avg. Training Loss: 0.06976, Avg. Validation Loss: 0.08902\n",
            "Train Steps: 2100, Avg. Training Loss: 0.06031, Avg. Validation Loss: 0.08806\n",
            "Train Steps: 2200, Avg. Training Loss: 0.06519, Avg. Validation Loss: 0.09024\n",
            "Train Steps: 2300, Avg. Training Loss: 0.06355, Avg. Validation Loss: 0.09003\n",
            "Train Steps: 2400, Avg. Training Loss: 0.06944, Avg. Validation Loss: 0.08811\n",
            "Train Steps: 2500, Avg. Training Loss: 0.06670, Avg. Validation Loss: 0.08763\n",
            "Train Steps: 2600, Avg. Training Loss: 0.06269, Avg. Validation Loss: 0.08668\n",
            "Train Steps: 2700, Avg. Training Loss: 0.05160, Avg. Validation Loss: 0.08684\n",
            "Train Steps: 2800, Avg. Training Loss: 0.06237, Avg. Validation Loss: 0.08637\n",
            "Train Steps: 2900, Avg. Training Loss: 0.06476, Avg. Validation Loss: 0.08495\n",
            "Train Steps: 3000, Avg. Training Loss: 0.06094, Avg. Validation Loss: 0.08548\n",
            "Train Steps: 3100, Avg. Training Loss: 0.05520, Avg. Validation Loss: 0.08490\n",
            "Train Steps: 3200, Avg. Training Loss: 0.06093, Avg. Validation Loss: 0.08559\n",
            "Train Steps: 3300, Avg. Training Loss: 0.05645, Avg. Validation Loss: 0.08452\n",
            "Train Steps: 3400, Avg. Training Loss: 0.05786, Avg. Validation Loss: 0.08526\n",
            "Train Steps: 3500, Avg. Training Loss: 0.05799, Avg. Validation Loss: 0.08675\n",
            "Train Steps: 3600, Avg. Training Loss: 0.05955, Avg. Validation Loss: 0.08358\n",
            "Train Steps: 3700, Avg. Training Loss: 0.06297, Avg. Validation Loss: 0.08535\n",
            "Train Steps: 3800, Avg. Training Loss: 0.06405, Avg. Validation Loss: 0.08390\n",
            "Train Steps: 3900, Avg. Training Loss: 0.06742, Avg. Validation Loss: 0.08477\n",
            "Train Steps: 4000, Avg. Training Loss: 0.05218, Avg. Validation Loss: 0.08471\n",
            "Train Steps: 4100, Avg. Training Loss: 0.05835, Avg. Validation Loss: 0.08447\n",
            "Train Steps: 4200, Avg. Training Loss: 0.06554, Avg. Validation Loss: 0.08271\n",
            "Train Steps: 4300, Avg. Training Loss: 0.05956, Avg. Validation Loss: 0.08366\n",
            "Train Steps: 4400, Avg. Training Loss: 0.06319, Avg. Validation Loss: 0.08340\n",
            "Train Steps: 4500, Avg. Training Loss: 0.05695, Avg. Validation Loss: 0.08339\n",
            "Train Steps: 4600, Avg. Training Loss: 0.06544, Avg. Validation Loss: 0.08517\n",
            "Train Steps: 4700, Avg. Training Loss: 0.05898, Avg. Validation Loss: 0.08429\n",
            "Train Steps: 4800, Avg. Training Loss: 0.05948, Avg. Validation Loss: 0.08240\n",
            "Train Steps: 4900, Avg. Training Loss: 0.05310, Avg. Validation Loss: 0.08314\n",
            "Train Steps: 5000, Avg. Training Loss: 0.05162, Avg. Validation Loss: 0.08311\n",
            "Train Steps: 5100, Avg. Training Loss: 0.06127, Avg. Validation Loss: 0.08303\n",
            "Train Steps: 5200, Avg. Training Loss: 0.05319, Avg. Validation Loss: 0.08383\n",
            "Train Steps: 5300, Avg. Training Loss: 0.05879, Avg. Validation Loss: 0.08177\n",
            "Train Steps: 5400, Avg. Training Loss: 0.05106, Avg. Validation Loss: 0.08290\n",
            "Train Steps: 5500, Avg. Training Loss: 0.05722, Avg. Validation Loss: 0.08346\n",
            "Train Steps: 5600, Avg. Training Loss: 0.05859, Avg. Validation Loss: 0.08382\n",
            "Train Steps: 5700, Avg. Training Loss: 0.05376, Avg. Validation Loss: 0.08212\n",
            "Train Steps: 5800, Avg. Training Loss: 0.05982, Avg. Validation Loss: 0.08143\n",
            "Train Steps: 5900, Avg. Training Loss: 0.05978, Avg. Validation Loss: 0.08343\n",
            "Train Steps: 6000, Avg. Training Loss: 0.06723, Avg. Validation Loss: 0.08162\n",
            "Train Steps: 6100, Avg. Training Loss: 0.06729, Avg. Validation Loss: 0.08152\n",
            "Train Steps: 6200, Avg. Training Loss: 0.05593, Avg. Validation Loss: 0.08137\n",
            "Train Steps: 6300, Avg. Training Loss: 0.05483, Avg. Validation Loss: 0.08086\n",
            "Train Steps: 6400, Avg. Training Loss: 0.05641, Avg. Validation Loss: 0.08143\n",
            "Train Steps: 6500, Avg. Training Loss: 0.05443, Avg. Validation Loss: 0.08072\n",
            "Train Steps: 6600, Avg. Training Loss: 0.06312, Avg. Validation Loss: 0.08087\n",
            "Train Steps: 6700, Avg. Training Loss: 0.05575, Avg. Validation Loss: 0.08130\n",
            "Train Steps: 6800, Avg. Training Loss: 0.05861, Avg. Validation Loss: 0.08235\n",
            "Train Steps: 6900, Avg. Training Loss: 0.05396, Avg. Validation Loss: 0.08050\n",
            "Train Steps: 7000, Avg. Training Loss: 0.05971, Avg. Validation Loss: 0.08102\n",
            "Train Steps: 7100, Avg. Training Loss: 0.05151, Avg. Validation Loss: 0.08318\n",
            "Train Steps: 7200, Avg. Training Loss: 0.05921, Avg. Validation Loss: 0.08047\n",
            "Train Steps: 7300, Avg. Training Loss: 0.05081, Avg. Validation Loss: 0.08013\n",
            "Train Steps: 7400, Avg. Training Loss: 0.05141, Avg. Validation Loss: 0.08137\n",
            "Train Steps: 7500, Avg. Training Loss: 0.06275, Avg. Validation Loss: 0.07975\n",
            "Train Steps: 7600, Avg. Training Loss: 0.05753, Avg. Validation Loss: 0.07999\n",
            "Train Steps: 7700, Avg. Training Loss: 0.05969, Avg. Validation Loss: 0.08152\n",
            "Train Steps: 7800, Avg. Training Loss: 0.06089, Avg. Validation Loss: 0.08344\n",
            "Train Steps: 7900, Avg. Training Loss: 0.05094, Avg. Validation Loss: 0.08214\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [1:04:52<00:00, 3892.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Steps: 8000, Avg. Training Loss: 0.05194, Avg. Validation Loss: 0.08171\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(LOSS, label = \"training loss\")\n",
        "plt.plot(VAL_LOSS, label = \"validation loss\")\n",
        "plt.legend()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "nvxlfPJc1q2v",
        "outputId": "bc4b662c-7bdd-45ac-8dd6-bb9fc78e6b8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7df714e180d0>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhYAAAGdCAYAAABO2DpVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABJfElEQVR4nO3dd3hTZeM+8DujSfeiW9pSoFBWWYVaQEUEWSLiQJFXwYELBF8EBRVE/SmofH3FhRscIIoyFBmykV1G2ZTV0gIdtKW7TdLk+f0ReiC0habNoKf357pyNck5Ped5mrS5+6yjEEIIEBEREdmA0tkFICIiIvlgsCAiIiKbYbAgIiIim2GwICIiIpthsCAiIiKbYbAgIiIim2GwICIiIpthsCAiIiKbUTv6hCaTCRcuXICXlxcUCoWjT09ERER1IIRAUVERwsLCoFTW3C7h8GBx4cIFhIeHO/q0REREZAPp6elo2rRpjdsdHiy8vLwAmAvm7e3t6NMTERFRHRQWFiI8PFz6HK+Jw4NFZfeHt7c3gwUREVEDc6NhDBy8SURERDbDYEFEREQ2w2BBRERENuPwMRZERGQ7QghUVFTAaDQ6uyjUwKlUKqjV6novBcFgQUTUQOn1emRkZKC0tNTZRSGZcHd3R2hoKDQaTZ2PwWBBRNQAmUwmpKSkQKVSISwsDBqNhosOUp0JIaDX63Hx4kWkpKQgOjr6uotgXQ+DBRFRA6TX62EymRAeHg53d3dnF4dkwM3NDS4uLjh79iz0ej1cXV3rdBwO3iQiasDq+l8lUXVs8X7iO5KIiIhshsGCiIgarGbNmuHjjz+u9f6bNm2CQqFAfn6+3coEAPPnz4evr69dz3Gz4hgLIiJymN69e6NTp05WhYHrSUxMhIeHR63379GjBzIyMuDj42OT81NVDBZERHRTEULAaDRCrb7xR1RgYKBVx9ZoNAgJCalr0agWZNMV8tE/yXhz+WFkFZY7uyhERFSN0aNHY/PmzZgzZw4UCgUUCgVSU1Ol7olVq1aha9eu0Gq12Lp1K06fPo2hQ4ciODgYnp6e6NatG9atW2dxzGu7QhQKBb799lsMGzYM7u7uiI6Oxp9//iltv7YrpLLLYs2aNWjTpg08PT0xYMAAZGRkSN9TUVGB8ePHw9fXF02aNMGrr76KUaNG4b777rOq/nPnzkWLFi2g0WjQunVr/PTTT9I2IQRmzJiBiIgIaLVahIWFYfz48dL2L774AtHR0XB1dUVwcDAefPBBq87tSLIJFr8kpuOHHWeRW6x3dlGIiBxOCIFSfYVTbkKIWpVxzpw5SEhIwJgxY5CRkYGMjAyEh4dL26dMmYJZs2bh2LFjiI2NRXFxMQYNGoT169dj//79GDBgAIYMGYK0tLTrnuett97C8OHDcfDgQQwaNAgjR45EXl5ejfuXlpZi9uzZ+Omnn7BlyxakpaVh0qRJ0vb3338fCxYswLx587Bt2zYUFhZi2bJltapzpaVLl2LChAl4+eWXcfjwYTz77LN44oknsHHjRgDAH3/8gf/973/46quvcPLkSSxbtgwdOnQAAOzZswfjx4/H22+/jeTkZKxevRq33367Ved3JNl0haguLwxjquUbnIhITsoMRrSdvsYp5z76dn+4a278ceLj4wONRgN3d/dquyPefvtt9OvXT3rs7++Pjh07So/feecdLF26FH/++SfGjRtX43lGjx6NESNGAADee+89fPLJJ9i9ezcGDBhQ7f4GgwFffvklWrRoAQAYN24c3n77bWn7p59+iqlTp2LYsGEAgM8++wwrV668YX2vNnv2bIwePRovvPACAGDixInYuXMnZs+ejTvvvBNpaWkICQlB37594eLigoiICHTv3h0AkJaWBg8PD9xzzz3w8vJCZGQkOnfubNX5HUk2LRYqpTlYGE0MFkREDVFcXJzF4+LiYkyaNAlt2rSBr68vPD09cezYsRu2WMTGxkr3PTw84O3tjezs7Br3d3d3l0IFAISGhkr7FxQUICsrS/qQB8zX1OjatatVdTt27Bh69uxp8VzPnj1x7NgxAMBDDz2EsrIyNG/eHGPGjMHSpUtRUVEBAOjXrx8iIyPRvHlzPPbYY1iwYMFNvYy7bFosKtf0MLLFgogaITcXFY6+3d9p57aFa2d3TJo0CWvXrsXs2bPRsmVLuLm54cEHH4Ref/0ubxcXF4vHCoUCJpPJqv1r271jK+Hh4UhOTsa6deuwdu1avPDCC/jwww+xefNmeHl5Yd++fdi0aRP++ecfTJ8+HTNmzEBiYuJNOaVVPi0WlV0hbLEgokZIoVDAXaN2ys2aa5RoNJpaX4l127ZtGD16NIYNG4YOHTogJCQEqampdfwJ1Y2Pjw+Cg4ORmJgoPWc0GrFv3z6rjtOmTRts27bN4rlt27ahbdu20mM3NzcMGTIEn3zyCTZt2oQdO3bg0KFDAAC1Wo2+ffvigw8+wMGDB5GamooNGzbUo2b2I6MWC/Mbu4LBgojoptWsWTPs2rULqamp8PT0hL+/f437RkdHY8mSJRgyZAgUCgWmTZt23ZYHe3nxxRcxc+ZMtGzZEjExMfj0009x6dIlqwLV5MmTMXz4cHTu3Bl9+/bFX3/9hSVLlkizXObPnw+j0Yj4+Hi4u7vj559/hpubGyIjI7FixQqcOXMGt99+O/z8/LBy5UqYTCa0bt3aXlWuF9m0WKiVbLEgIrrZTZo0CSqVCm3btkVgYOB1x0t89NFH8PPzQ48ePTBkyBD0798fXbp0cWBpzV599VWMGDECjz/+OBISEuDp6Yn+/ftbdZGu++67D3PmzMHs2bPRrl07fPXVV5g3bx569+4NAPD19cU333yDnj17IjY2FuvWrcNff/2FJk2awNfXF0uWLEGfPn3Qpk0bfPnll/jll1/Qrl07O9W4fhTCwR1JhYWF8PHxQUFBAby9vW123AEfb8HxzCL89FR33BZt3YIpREQNTXl5OVJSUhAVFVXnq1BS3ZhMJrRp0wbDhw/HO++84+zi2NT13le1/fyWTVcIZ4UQEZE9nD17Fv/88w/uuOMO6HQ6fPbZZ0hJScGjjz7q7KLdlGTTFVIZLLiOBRER2ZJSqcT8+fPRrVs39OzZE4cOHcK6devQpk0bZxftpiSbFgulorLFwskFISIiWQkPD68yo4NqJrsWC3aFEBEROY98ggWX9CYiInI62QQLaeVNtlgQERE5jWyCBQdvEhEROZ9sgsWVwZsMFkRERM4im2DBwZtERETOJ59gwcGbRESNQrNmzfDxxx9LjxUKBZYtW1bj/qmpqVAoFEhKSqrXeW11nBsZPXo07rvvPruew57ks46FkutYEBE1RhkZGfDz87PpMUePHo38/HyLwBIeHo6MjAwEBATY9FxyI5tgUdliYWSLBRFRoxISEuKQ86hUKoedqyGTT1cIr25KRHRT+/rrrxEWFlbl0udDhw7Fk08+CQA4ffo0hg4diuDgYHh6eqJbt27SpcVrcm1XyO7du9G5c2e4uroiLi4O+/fvt9jfaDTiqaeeQlRUFNzc3NC6dWvMmTNH2j5jxgz88MMPWL58ORQKBRQKBTZt2lRtV8jmzZvRvXt3aLVahIaGYsqUKaioqJC29+7dG+PHj8crr7wCf39/hISEYMaMGVb93HQ6HcaPH4+goCC4urqiV69eSExMlLZfunQJI0eORGBgINzc3BAdHY158+YBAPR6PcaNG4fQ0FC4uroiMjISM2fOtOr81pJNi4WSgzeJqDETAjCUOufcLu7A5Vbj63nooYfw4osvYuPGjbjrrrsAAHl5eVi9ejVWrlwJACguLsagQYPw7rvvQqvV4scff8SQIUOQnJyMiIiIG56juLgY99xzD/r164eff/4ZKSkpmDBhgsU+JpMJTZs2xeLFi9GkSRNs374dzzzzDEJDQzF8+HBMmjQJx44dQ2FhofQB7e/vjwsXLlgc5/z58xg0aBBGjx6NH3/8EcePH8eYMWPg6upqER5++OEHTJw4Ebt27cKOHTswevRo9OzZE/369bthfQDglVdewR9//IEffvgBkZGR+OCDD9C/f3+cOnUK/v7+mDZtGo4ePYpVq1YhICAAp06dQllZGQDgk08+wZ9//onffvsNERERSE9PR3p6eq3OW1eyCRaqy+9pDt4kokbJUAq8F+acc792AdB43HA3Pz8/DBw4EAsXLpSCxe+//46AgADceeedAICOHTuiY8eO0ve88847WLp0Kf7880+MGzfuhudYuHAhTCYTvvvuO7i6uqJdu3Y4d+4cnn/+eWkfFxcXvPXWW9LjqKgo7NixA7/99huGDx8OT09PuLm5QafTXbfr44svvkB4eDg+++wzKBQKxMTE4MKFC3j11Vcxffp0KC+v3BgbG4s333wTABAdHY3PPvsM69evr1WwKCkpwdy5czF//nwMHDgQAPDNN99g7dq1+O677zB58mSkpaWhc+fOiIuLA2Ae3FopLS0N0dHR6NWrFxQKBSIjI294zvqSTVcIWyyIiG5+I0eOxB9//AGdTgcAWLBgAR555BHpQ7i4uBiTJk1CmzZt4OvrC09PTxw7dgxpaWm1Ov6xY8cQGxsLV1dX6bmEhIQq+33++efo2rUrAgMD4enpia+//rrW57j6XAkJCVBc1VrTs2dPFBcX49y5c9JzsbGxFt8XGhqK7OzsWp3j9OnTMBgM6Nmzp/Sci4sLunfvjmPHjgEAnn/+eSxatAidOnXCK6+8gu3bt0v7jh49GklJSWjdujXGjx+Pf/75x6o61oWMWiw4eJOIGjEXd3PLgbPOXUtDhgyBEAJ///03unXrhn///Rf/+9//pO2TJk3C2rVrMXv2bLRs2RJubm548MEHodfrbVbcRYsWYdKkSfi///s/JCQkwMvLCx9++CF27dpls3NczcXFxeKxQqGoMs6kPgYOHIizZ89i5cqVWLt2Le666y6MHTsWs2fPRpcuXZCSkoJVq1Zh3bp1GD58OPr27Yvff//dZue/lnyCBQdvElFjplDUqjvC2VxdXXH//fdjwYIFOHXqFFq3bo0uXbpI27dt24bRo0dj2LBhAMwtGKmpqbU+fps2bfDTTz+hvLxcarXYuXOnxT7btm1Djx498MILL0jPnT592mIfjUYDo9F4w3P98ccfEEJIrRbbtm2Dl5cXmjZtWusyX0+LFi2g0Wiwbds2qRvDYDAgMTERL730krRfYGAgRo0ahVGjRuG2227D5MmTMXv2bACAt7c3Hn74YTz88MN48MEHMWDAAOTl5cHf398mZbyW7LpCKhgsiIhuaiNHjsTff/+N77//HiNHjrTYFh0djSVLliApKQkHDhzAo48+atV/948++igUCgXGjBmDo0ePYuXKldIH7NXn2LNnD9asWYMTJ05g2rRpFrMsAPM4hYMHDyI5ORk5OTkwGAxVzvXCCy8gPT0dL774Io4fP47ly5fjzTffxMSJE6Wunfry8PDA888/j8mTJ2P16tU4evQoxowZg9LSUjz11FMAgOnTp2P58uU4deoUjhw5ghUrVqBNmzYAgI8++gi//PILjh8/jhMnTmDx4sUICQmBr6+vTcpXHdkECzVbLIiIGoQ+ffrA398fycnJePTRRy22ffTRR/Dz80OPHj0wZMgQ9O/f36JF40Y8PT3x119/4dChQ+jcuTNef/11vP/++xb7PPvss7j//vvx8MMPIz4+Hrm5uRatFwAwZswYtG7dGnFxcQgMDMS2bduqnOuWW27BypUrsXv3bnTs2BHPPfccnnrqKbzxxhtW/DRubNasWXjggQfw2GOPoUuXLjh16hTWrFkjLQqm0WgwdepUxMbG4vbbb4dKpcKiRYsAAF5eXvjggw8QFxeHbt26ITU1FStXrrRZ8KmOQgjHDkooLCyEj48PCgoK4O3tbbPjzvjzCOZvT8XYO1tgcv8Ymx2XiOhmVF5ejpSUFERFRVkMVCSqj+u9r2r7+V2vyDJr1iwoFAqLfh5nUXFJbyIiIqerc7BITEzEV199VWUajbNIgzc5K4SIiMhp6hQsiouLMXLkSHzzzTc2v/BLXSkVXMeCiIjI2eoULMaOHYvBgwejb9++N9xXp9OhsLDQ4mYPqss1YbAgIiJyHqvXsVi0aBH27dtXZWpOTWbOnGmxdKq9VC6Qxa4QIiIi57GqxSI9PR0TJkzAggULaj0KeerUqSgoKJBu9rr4CZf0JqLGyMET+0jmbPF+sqrFYu/evcjOzraYU2w0GrFlyxZ89tln0Ol0UKlUFt+j1Wqh1WrrXdAbYYsFETUmlctEl5aWws3NzcmlIbkoLTVfIffaZcitYVWwuOuuu3Do0CGL55544gnExMTg1VdfrRIqHIktFkTUmKhUKvj6+koXs3J3d7e4GBaRNYQQKC0tRXZ2Nnx9fev1eW5VsPDy8kL79u0tnvPw8ECTJk2qPO9oXMeCiBqbykt61/ZKmUQ34uvre91LxdeGfC5Cxq4QImpkFAoFQkNDERQUVO21LIis4eLiYpOeh3oHi02bNtW7ELbArhAiaqxUKpVTu6KJriabi5CpLnctGtliQURE5DTyCRa8uikREZHTySZYsCuEiIjI+WQTLDh4k4iIyPlkEyzYYkFEROR8sgkWlS0WRuYKIiIip5FPsODgTSIiIqeTXbCoMHHpTSIiImeRXbBgriAiInIe2QQLpTTGgl0hREREziKbYKHirBAiIiKnk1GwMH/lOhZERETOI5tgIXWFsMWCiIjIaWQTLNgVQkRE5HzyCRZc0puIiMjpZBMsuKQ3ERGR88kmWEjrWDBXEBEROY1sggUHbxIRETmfbIIFB28SERE5n3yCBQdvEhEROZ1sgoXyck3YYkFEROQ8sgkWVwZvMlgQERE5i3yCBQdvEhEROZ1sggXXsSAiInI+2QSLK4M3nVwQIiKiRkw+wYItFkRERE4nm2AhdYVw8CYREZHTyCZYcPAmERGR88knWLArhIiIyOlkFywAwMRwQURE5BTyCRaKK8GC4yyIiIicQzbBQnlVTdgdQkRE5ByyCRYWXSFssSAiInIK2QQL5dVdIWyxICIicgrZBAvLwZtOLAgREVEjJp9gwcGbRERETiebYKFUsiuEiIjI2WQTLIAr3SEcvElEROQc8goWXNabiIjIqWQVLCrXsmCwICIicg5ZBYvKFgt2hRARETmHrIKFkhciIyIicipZBQsO3iQiInIueQULafCmkwtCRETUSMkqWLArhIiIyLlkFSw4eJOIiMi55BUs2GJBRETkVLIKFtI6FmyxICIicgpZBQv15WTBFgsiIiLnkFWwqPTH3nPOLgIREVGjJKtgkZJTAgBYlJju5JIQERE1TrIKFkRERORcsg0WJo6zICIicjjZBosSfYWzi0BERNToyDZYFJUzWBARETmarIJFz5ZNpPvFOgYLIiIiR5NVsPh+dDfpPlssiIiIHE9WwUKrVqFtqDcAoKjc4OTSEBERNT6yChYA4OmqBsCuECIiImeQXbDwrgwW7AohIiJyONkFCw8tWyyIiIicRXbBQqMyV0lvNDm5JERERI2P/IKF+nKwqGCwICIicjQGCyIiIrIZ+QWLy10hBnaFEBEROZxVwWLu3LmIjY2Ft7c3vL29kZCQgFWrVtmrbHXCFgsiIiLnsSpYNG3aFLNmzcLevXuxZ88e9OnTB0OHDsWRI0fsVT6rcfAmERGR86it2XnIkCEWj999913MnTsXO3fuRLt27WxasLpyudxioWOLBRERkcNZFSyuZjQasXjxYpSUlCAhIaHG/XQ6HXQ6nfS4sLCwrqeslStjLIRdz0NERERVWT1489ChQ/D09IRWq8Vzzz2HpUuXom3btjXuP3PmTPj4+Ei38PDwehX4Rq6MsTDa9TxERERUldXBonXr1khKSsKuXbvw/PPPY9SoUTh69GiN+0+dOhUFBQXSLT09vV4FvhFpjAW7QoiIiBzO6q4QjUaDli1bAgC6du2KxMREzJkzB1999VW1+2u1Wmi12vqV0pryqdkVQkRE5Cz1XsfCZDJZjKFwNk43JSIich6rWiymTp2KgQMHIiIiAkVFRVi4cCE2bdqENWvW2Kt8VqvsCtFxuikREZHDWRUssrOz8fjjjyMjIwM+Pj6IjY3FmjVr0K9fP3uVz2qV000NbLEgIiJyOKuCxXfffWevctgMF8giIiJyHvldK4RjLIiIiJxGfsGCFyEjIiJyGvkFC7ZYEBEROQ2DBREREdmM7IKFi0oBgIM3iYiInEF2wUJqsTCaIARX3yQiInIk+QWLy4M3hQCMJgYLIiIiR5JdsFCrrlSpgsGCiIjIoeQXLJQK6T6nnBIRETmW7IKFy9UtFrzCKRERkUPJLliolAooLjdaGExssSAiInIk2QULAHBRmqvFFgsiIiLHkmWwUF9ey4LBgoiIyLHkGSwuD+BkVwgREZFjyTJYVA7gZIsFERGRY8kyWFR2hXC6KRERkWPJM1hUDt7kAllEREQOJctg4SIN3mSLBRERkSPJMlhULutt4BgLIiIih5JnsFByjAUREZEzyDJYSLNCON2UiIjIoWQZLK7MCmFXCBERkSPJMlhwSW8iIiLnkGWwkJb0ZlcIERGRQ8kyWLhwVggREZFTyDRYcB0LIiIiZ5BlsKhcedPAlTeJiIgcSp7Bgi0WRERETiHLYMGrmxIRETmHLIOFtPImZ4UQERE5lDyDBVssiIiInEKWwYKzQoiIiJxDlsGCs0KIiIicQ5bBgi0WREREziHLYMGLkBERETmHLINF5XRTPVssiIiIHEqWwUKjvhwsKhgsiIiIHEmewUK6CBmDBRERkSPJMlho2WJBRETkFLIMFuwKISIicg55Bwt2hRARETmUPIOFSgUA0LHFgoiIyKHkGSzYFUJEROQUsgwWlStvMlgQERE5liyDBcdYEBEROYcsgwWnmxIRETmHLINF5eBNLpBFRETkWPIMFmyxICIicgoGCyIiIrIZWQcLHbtCiIiIHEqewUJ1pcVCCOHk0hARETUesg4WAGAwMlgQERE5ijyDhfpKtbiWBRERkePIP1hwACcREZHDyDJYqJQKqJTmZb25lgUREZHjyDJYAFfGWegMDBZERESOIttg4aFVAwBK9BVOLgkREVHjIdtg4eVqDhbFOgYLIiIiR5FtsPDQmq8XUlzOYEFEROQosg0Wnlq2WBARETmajIOFCwAGCyIiIkeSbbCQxliwK4SIiMhhZBssKsdYFLHFgoiIyGFkGyykrhC2WBARETmMbINFZVdICVssiIiIHEa2wYKzQoiIiBzPqmAxc+ZMdOvWDV5eXggKCsJ9992H5ORke5WtXipX3uQYCyIiIsexKlhs3rwZY8eOxc6dO7F27VoYDAbcfffdKCkpsVf56kxqsSg3OLkkREREjYfamp1Xr15t8Xj+/PkICgrC3r17cfvtt9u0YPV1ZYyF0cklISIiajysChbXKigoAAD4+/vXuI9Op4NOp5MeFxYW1ueUtcYxFkRERI5X58GbJpMJL730Enr27In27dvXuN/MmTPh4+Mj3cLDw+t6SqtIYyzYFUJEROQwdQ4WY8eOxeHDh7Fo0aLr7jd16lQUFBRIt/T09Lqe0ipXX91UCOGQcxIRETV2deoKGTduHFasWIEtW7agadOm191Xq9VCq9XWqXD1UdkVYhJAucEEN43K4WUgIiJqbKxqsRBCYNy4cVi6dCk2bNiAqKgoe5Wr3tw1KigU5vtFOnaHEBEROYJVLRZjx47FwoULsXz5cnh5eSEzMxMA4OPjAzc3N7sUsK4UCgU8NWoU6SpQXF6BIC9nl4iIiEj+rGqxmDt3LgoKCtC7d2+EhoZKt19//dVe5asXT1fODCEiInIkq1osGtogSE45JSIicizZXisEuKrFglc4JSIicgh5Bwu2WBARETlUowgWG5MvwmRqWN04REREDVGjCBZ/HbiA3/edc3JpiIiI5E/ewcL1ytjUvw5ccGJJiIiIGgd5BwvtlWDh4+bixJIQERE1Do0mWPi5a5xYEiIiosZB1sHCRXWler7ubLEgIiKyN1kHC73RJN3nRciIiIjsT9bBotxglO5zuikREZH9yTpY3BMbJt2vYLAgIiKyO1kHi5ZBnhjYPgQAUGFksCAiIrI3WQcLAAjzNV/OnS0WRERE9if7YKFWKgAARpPpBnsSERFRfck/WKjMwYItFkRERPYnn2Cx5BlgwUPApVSLp1VKcxWNDBZERER2J59gcWYTcPIfQFdk8XRlVwhbLIiIiOxPPsFCcbkqJqPF06rKMRacFUJERGR3MgoWl1fWFJaDNNliQURE5DgyChaXqyIsA4SKs0KIiIgcRkbBwhwgICy7QthiQURE5DjyCRbK6rtCVCrOCiEiInIU+QQLqSuk+jEWBg7eJCIisjvZBwuOsSAiInIc+QULE8dYEBEROYuMgkUN0005xoKIiMhhZBQsrj/Ggi0WRERE9iejYFE53bSmMRYMFkRERPYmn2BRw3RTtlgQERE5jnyCBWeFEBEROZ38gkWVWSHm5yu4jgUREZHdyS9YcIwFERGR08goWNQ03ZTBgoiIyFFkFCyu32Jh4BgLIiIiu5NRsKh+umnlrBAjx1gQERHZnXyCRU1XN+V0UyIiIoeRT7CooSvEhUt6ExEROYz8gsU1003ZYkFEROQ4MgoW119502A0QQiGCyIiInuSUbCovivE30MDjVqJUr0R/57McULBiIiIGg/ZBwsvVxfc1ykMALD9dK6jS0VERNSoyChYVE43NVbZFOztCgAo01c4skRERESNjnyChTTdtOo4CjeNeVupvmroICIiItuRT7CooSsEANxczMGizMBgQUREZE/yCxamquHB/XKLRRlbLIiIiOxKRsGi+ummAOCmUQNgVwgREZG9yShYsCuEiIjI2WQYLNgVQkRE5CzyCRbK67RYVM4KMXC6KRERkT3JJ1jUpitEX3UbERER2Y4Mg0XVdSyudIWwxYKIiMie5BcsqpluWtliUWow8kJkREREdiSjYHG96abmbUIAugp2hxAREdmLjIJFzWMs3C+vYwEAJTp2hxAREdmLDINF1a4QlVKBIC8tACD9UpkjS0VERNSoyCdYKGvuCgGAFoGeAIDT2cWOKhEREVGjI59gIV02vYZgEeQBADh9kcGCiIjIXmQULCpnhVQfLJo1MQeLtLxSR5WIiIio0ZFfsKihxcLHzQUAUFTOwZtERET2IqNgcf0xFl6u5pkhxZwVQkREZDcyChbXb7Hwcq1ssTA4qkRERESNjgyDRfVXMJVaLNgVQkREZDfyCRY3mG7qqTUHC46xICIish/5BIsbTDeVukJ0FTCZeL0QIiIie5BRsLj+dNPKrhAAeGP5YUeUiIiIqNGxOlhs2bIFQ4YMQVhYGBQKBZYtW2aHYtXBDWaFaNVXqrpwV5ojSkRERNToWB0sSkpK0LFjR3z++ef2KE/d3WBWiKKyq4SIiIjsRn3jXSwNHDgQAwcOtEdZ6ucGwYKIiIjsz+5jLHQ6HQoLCy1udnGD6aYAsPDpeACAh0ZlnzIQERE1cnYPFjNnzoSPj490Cw8Pt8+JbjDdFACig70AAKUGI2eGEBER2YHdg8XUqVNRUFAg3dLT0+1zolp0hXhozeFDCKC8ouaWDSIiIqobq8dYWEur1UKr1dr7NFfWsTDVHBjcXFRQKMzBokRnhLvG7tUnIiJqVGS0jkVlV0jNXRwKhQIel8NECS9GRkREZHNW/8teXFyMU6dOSY9TUlKQlJQEf39/RERE2LRwVqnlrBB3jQrFugqU6BksiIiIbM3qFos9e/agc+fO6Ny5MwBg4sSJ6Ny5M6ZPn27zwlmlFrNCAHOwAIA1R7LsXSIiIqJGx+oWi969e0Ncp7vBaWrZYpGaWwoA+GT9SUzs18repSIiImpU5DPGohbTTYmIiMi+5BMsatli8fmjXaT7FUaGECIiIluSX7C4znRTAOjXNli6X6LjWhZERES2JL9gcYMWC41aCY3KvG8xZ4YQERHZlHyChcrF/NVouOGulStwci0LIiIi25JPsFBeDham2gQL82SYYgYLIiIim5JPsFBdnjlrvHFY8LwcLJbtP2/PEhERETU68gkWVrRY6CvM4zB+3HEWZXoO4CQiIrIV+QQLK8ZYpF8qle7nFOvsVSIiIqJGRz7BwooWC4PxysqhFxksiIiIbEY+wcKKMRZxkX7S/dxivb1KRERE1OjIJ1hY0WLx6aOdpfvsCiEiIrId+QQLK8ZYhPq4YXhcUwBAThGDBRERka3IJ1goL3eF3GBJ70oBnloAwF8HL9ycV2slIiJqgOQTLFS17woBgMGxoQCAE1nFOJFVbK9SERERNSryCRbK2neFAEC7MB80a+IOAMgv5QBOIiIiW5BPsKhssRBGoJZdGz7uGgBAYTmX9iYiIrIF+QSLyjEWQK1bLbxdzd9TWFa7/YmIiOj65BMsKlssgFqPs/BxM3/Py4sPoNzApb2JiIjqSz7BQnlVsKhti4Xble/5dMNJW5eIiIio0ZFPsLBosajdmInKq5wCwPbTubYuERERUaMjn2ChUAAKlfl+LVssSnRXAsj5S2X2KBUREVGjIp9gAVi9lsWlq6aZZhfpUFDKQZxERET1Ia9gYeVaFo90i7B4fDyz0NYlIiIialTkFSwqr3BayzEWt7cKxNr/3o4eLZoAAJKziuxVMiIiokZBXsHCyhYLAIgO9kLHcF8AwPFMBgsiIqL6kFewsHKMRaWYEC8AQDKDBRERUb3IK1hUrr5ptG6J7pgQbwDAicwiXumUiIioHuQVLOrYYtE80AMuKgWKdBU4n89pp0RERHUlr2BRhzEWAOCiUqJFoCcA4HgGWy2IiIjqSl7BQpoVYv16FK0vj7N4+sc96P7eeuSV8FLqRERE1pJXsJBaLKy/DHqXCD/p/sUiHXae4RLfRERE1pJXsFBrzV+NOqu/dXBsKFRKhfRYV8GrnRIREVlLXsFCYx4nAV2x1d8a4KnFnEc6SY//PpgJk4ljLYiIiKwhr2ChNY+TgK5u61HcExuG/9xqXuZ73bEsLD9w3lYlIyIiahQYLK7h566R7v+042x9S0RERNSoyDRY1P1iYt6uLtJ99oQQERFZR57BQm/9GItKZYYrgzYvleo5iJOIiMgK8gwW9egKCfDUSvfP5pai9RursffspfqWjIiIqFFgsLjGA11vweMJkRbPfb81pT6lIiIiajQYLK49hFqFt4e2x8Ix8dJzZ3JK6lsyIiKiRkGmwaLugzcr9WgRgPUv3wEAOJZRiGKd9at5EhERNTbyChaa+rdYXC3cz1263+v9DSjTcyAnERHR9cgrWLj5mr+W2WawpUatRIdbfAAA+aUGrD2WZZPjEhERyZW8goV7E/PX8oI6XYisOj8/FY/bogMAAN9tTcGIr3diT2qeTY5NREQkN/IKFq6+V+6X59vkkD7uLhjQPgQAcCA9HzvO5OKrLWdscmwiIiK5kVewUKkBV3PXBUpt16pQ2R1Sae3RLJy5WPdFuIiIiORKXsECANz8zV/LbBssRsZHWDw3Zckhmx2fiIhILuQXLNwvBwsbtlgoFAq8O6wDkv/fAPi4ma8lsjslD0ZeTISIiMiC/IJFZYtFaa7ND61Vq7Dnjb5QKxUAgCMXChguiIiIriK/YFE5M6Q0xy6Hd1EpcXurQADAvZ9tw/CvdtjlPERERA2R/IKFZ5D5a/FFu53imdubS/f3nr2E3GKd3c5FRETUkMgvWHiZp4ai2H6LWcVH+Vs87v/xv9BXmOx2PiIiooZCfsHCM9j81Y7BQqFQYNb9HaTHOcU6LEpMs9v5iIiIGgoZBovKrpBsu57mke4ReGVAa+nxigMZdj0fERFRQyDDYGH/FotKT/WKwvsPmFsudqfm4aN/ku1+TiIiopuZ2tkFsLnKMRbl+UB5IeDqbbdTadUqDI8Lx/TlR6CrMOGTDadwPLMI5y6VQeuixM9PxcNDK78fMRERUU3k12Lh6gN4Xg4XF+3fgqBQKPDtqDjp8T9Hs3A0oxD70/Lxy+40lOptczE0IiKihkB+wQIAgmLMXy8ed8jpbosOxMrxt+HutsHo1TJAev7//X0Mvd7fiDeXH8aF/DKHlIWIiMiZ5BksAtuYv57f47BTtg3zxtePx+Hnp+Px27MJ0vN5JXr8sOMsnv1pLwrKDFypk4iIZE2ewaL1APPXg78BWUcdfvouEb5Vnjt0vgAd3/oHLV5biZ6zNuDIhQKHl8taJboKGIy2XZ/jUokeB9LzAQBCCOw4ncvuoqsIIXCxiAuu0c1JCIFjGYXQVRidXRS6ickzWETdAdwSBxhKga97A4tHA6tfAxK/BU6tBzIOAmd3ACfXAZdSAX0JYLrmF0XUvWVBrVLi7aHt0L2ZP14bFFNl+/n8Mry25BAMRhPWHs3CgI+3YP62FMzbloL9aZeqPWZqTglGfrsT207lILuoHOUGIx77bhf6zN6EEt2NP5iFEFaFhLTcUnR/dx3++2tSlW3lBiN+3JGK5MyiGhcGO3guH3vPXqnL9lM52HUmFw99tQNDP9+GxNQ8rDyUiRHf7MSzP+2tsRypOSUY8PEWLNl3rtZlb8h+2J6Kbu+uwx97G0d9r1WmN0LU4XfPZBIQQuBCfhnO5pbYoWQNW6m+Ap+uP4nzteySFULg2Z/2YNT3uy1+x1cczMDAOf/ijaWH7VLOwnIDdp3JrdN74GaSXViOacsOIzmzyNlFcQqFcPArWFhYCB8fHxQUFMDb234zNlCSCywZA5xeX7v9lWpA62UOFEoVUJZvvlKq2g1QKC5fNVVhXh/DMxBQuwJGA+DiBhjKALXWfF/tCmi9AaUSUGmA8kLoocaWA8nIEObrmBigRowiDcVKTxw1NsXHFQ9WKY6PmwtaBnni/93XHqX6Cvxv7UlsPVX99U/mPNIJQzvdUuX5U9nFeG3pIdzWMgAuaiU+XJOMhU/Hw02jwmcbTmFghxAM69y02mO+tvQQFu4yL/p1+r1BMBhNeG/lMdwZE4QVBzLwx1Uf9F/+pysGtA+BySSgUABlBiPaTl8DANjzRl8kZxZh5Le7LI7/n1sjcDq7BDvOmC8Wl/h6X6TllWDH6VwcuVCIjx/pBK1ahSfnJ2LD8WzpWMv2n8fADqG4xdetSpmX7T+Ply4HoW8ej0O/tsHILirHb4npeCyhmXRl2p1ncvHI1zsBAA91bYrXB7eBr7sGJpPAgl1nkX6pDBP7tYKri6rKOUr1FSgsq0CQlxZKpQIVRhOWJV1A10g/RAV4AABWH87Evycv4o3BbVFQZkCIj2uV45ToKuDqosKRCwV4felhTLunLbpH+aPZlL+lfVJnDa72tamLw+cL4O3qgogm7jh0rgD/HM3Ei32iYTCapJlL+aV6LNiVhke7R8DPQ4PswnK4aVQoKDPgyfmJ6BMTjKJyA3aczsVbQ9shwt8dfh4aeLu6VKnbvycvokfLAHi7usBgNGH9sWyk55ViQPsQbD5xEUXlFejVMgAtgjzgrjGff1/aJTzy1U480asZpg5sA5NJoMIkoFFb/v9TVG6Am4sKapX5+ezCcgyY8y8SmjfB2qNZ0BtNWDa2J2JCvKp9DW9WQghcKjXA30MjPVeqr8CSfedxR6tAqJQKhFXzvgcAg9EEF1XN/ydW/j5H+Ltjyyt3AgCmLTuMlJwSvP9gLNJyS3Frc3+cyCrGl5tPo3WIF2atMo9Re753C/RqGYCeLQPQY+Z6XCgoBwAcmnE3vK557U9kFWHassOY3L814pqZVyj+avNp5JcZ8Er/1lAoFFXqnJxVhKX7z2N0j2aY8ecRrDmShbfubYeUnBKM6B6B1iFeVv4kzTYmZ+O5n/bigwdj0TXSD+uOZmHkrZHSzymjoAy/7E7HmNui4OXqglPZRfhgdTJK9BWYfk876by7U/KgrzChV3TA9U5nYeyCffj7UAa8tGoceqt/jful55UiNbcEczedRotAT7w9tF2Vn1FWYTkGf7IV98SGYsa97erwk7Cd2n5+yzdYAOaQcC4ROLHGfLXTC/vMgcNkMG9z9QHyzwJGvX3LcR27Ta0xXP9mvY/TKdwXn47ojPP5Zcgp1kGrVmHMj9WPMXFzUaHMYG6h6dbMD0cvFCLQS4tSvRGP3RqJgR1C0PejLdL+o3s0w4qDGci5zjVRJtwVjU82nKxPQ4+FZ+9ojmMZRdhy4so1X+Ii/bDn7CU08dBgxfheyC3W441lh3HmYjFG94zCJ+tPWhzj4bhw/LonHQAQ2cQdUwbE4O9DGVhxsOpiZn+N64WcYh2emJ8IAHh3WHuMjI+EEAJbT+Vgw/FsjO8TjQm/JkllahXsCS9XF+w9ewlhPq547/4OaBvmje7vmsNsqI8rMgrK8XSvKAxoHwJ9hQktgz2RXajDsC+2YUT3CKw8lImcYh00aiUm390a7648JpVpYr9W6Bjui9ujA5CaW4q8Eh3OXSrD73vPYca97bD9dC52p+QhNacEtzb3xz9Hs6BSKjD57tbIK9XDUGHC0E634O9DGXhjmfk/zHlPdMMT88x1bB3sheQs839Ut/i6Wfw3O+nuVvi/tScQ6KmFt5sLTmUXV/s6dWvmhwVP3yp9+OsrTBg4ZwtOXyzBqIRIvDW0PT5YfRxfbDpd42u9fUofhPm6WXxoDeoQgjMXS3AyuxhNPDSIb94EM+/vgNScEgz7YhuGx4Wj2+UPrl8T06WAWp17O4aha6Qfdp7JxYjuETh3qQyrj2Qiv1SP+Ch/TOjbCq5qJf4+lIEuEX4I93e3+P6icgPUSiWWJ53HumPZeOb25tiYnA2VQoEWQR6Yv/0s7ooJwtBOYXj+531oFeyJ9+7vgAPpBZi65CD8PDQo0xsxdVAb/H3wAnzdNegc7osWQZ5oFXzlQ3P+thTM+MvcdTugXQi0LkoUlhmwMdn8flMrFfh7/G1oFeyJU9nF8HXXINBLi43Hs/HE/ES8M7QdBnYIhUathJdWDb3RhH9P5OBoRiHmrD8pje96Y3AbbD2Vg03JltdT8nJVo6i85tbPyf1b48M1V2baKRXA78/3QOdwX2w5mYM2IV4Y8tlWZBXq0MRDg02Te2PFwQxMXXJIOn6XCD90DPfFxH6tIITAzFXH8fWWMwCA5oEeOHPRsrUp2FuLXa/1RVJ6PjYnX4SHVoWD5wow4952+PfkRWQX6qBSKvDVltPw0Kox7s6WuL+L+Z+lq0N6uL8b0vPM7++fnuqOvBI9JixKAgBEB3liysAYvLPiKFJzSwEALQI9sP7l3rhUokfnd9YCAHZM7YNQn6rBLrOgHC4qBTy0ahzLKMT5/DKMW7hf2j6kYxh0BiNOXSzG64PaoEeLADz45XZ4aNXYd/YSKq4ad/f+Ax1gMAo0D/SASqHA11vOIK9Uj/1p+QCAzZN7I8LfXQofRpNAcXkFfNxdcORCAYK9XRHgqa3xNawvBovaEsLcFVJ2yfxVmABThTl8VOjMLRkmI1CSbW6BcPUBijIBhdLcklHZsqEvMa+bIUxARZn5GMYKQOMOGMqQrzOhqOASmvq4YvGeszisCwKMeuQJb6wwJVQpVpiPq/RH9mrxUf54qlcUliddwN+HuNqnvQzrfAvuaBUotYDYirerGjGh3tidkmfT4zpTsLcWj90aie2nc1FmMEp/BAHzH+jTF2/cNdEm1BvHMgrtWMqaDe4QinKDEesvt4zd2twfXSL8UKo3ok9MEF5efKBW41583V2QX2oAYG4JW3yD7iyFwryfv4cWucW6G+4PAP3bBSO/1IBdl98/3q5qFF4nDDiSu0aFUv2VLuWB7UOw6nBmtfsO7RSG5UkXanXcAE/tdf+pqU5TPzecu1S/mXiPJ0Tixx1npcdvDmmLW5s3QUGZAaE+rjAYBXan5OG1pYegUACBnlpkO2B8VPMAD/RsGYCukX7437oTOJtbavGzbxnkiakDY3BHq0CpVc9W7BosPv/8c3z44YfIzMxEx44d8emnn6J79+42LZiclemNEBDILzXg5d8OAADO5BTjYpEOrUO8MbFfK/RrG4z7v9iGA+cK8MztzTF302loVErsmdZXanrWV5iweG86Ajy1GLdwHwxG22bEhOZNrvuf4LX8PTTwclXj7OXUX2naPW2xL+0S/q6mpcBarw6Iwfurr0wjDvNxhUKhsPhve3L/1vh0w0m0DvbChYJyuw2G7NmyCSL83bE/LR/HZdiXqlIq0DbUG4fOWw409tSqUVyLcT1E5Dx/PN8DXSP9bHpMuwWLX3/9FY8//ji+/PJLxMfH4+OPP8bixYuRnJyMoKAgmxWMzH3eeSV6NA/0xPZTOYAC6NGi+n6+DcezMH/7WTzRsxmyCsqxLOk8Dp4rQHSQJ35+Oh4frklGXDN/tA31RmZBOTpH+CIlpwTeri4I93eDEMDryw7hl93mroNRCZGYPqQdlArzImBCCJy+WIxl+y/AYDTh+d4tcPh8IX7bk45/jmbCz12DjZN6w9VFBSEE0vPKMPn3A+ge5Y+J/VpBoVBAX2HCH/vOYem+87i/yy0oMxix5cRFbEy+iFAfVzzfuwWmLz8CABh3Z0s80bMZElPz8OGaZBSUVeCHJ7uhXZgPxi7Yh80nLuLFPi3xzO3NcfpiMYZ9sR1F5RWY3L81xt7ZEhVGk0Va/2PvORy5UIiX+kWjwiiQV6JDUnoBcop16Brph/dWHpP+0/bSqlFUzQdni0APuLqoMLB9CIZ3C0eQl3nsRE6xDo98vROFZQbMuLcd+rUNRkGZAeuPZeFYRhHmb0/F0E5hyCvR49+TluNk/D00aBvqja2nctA9yh8/PdUd//l2FxJTL8HNRYXoYE/kFushhMBTtzXHwXP5WH8sG0M6hiG3WIcdZ3Kl5usATy0+eaQTHr1mPIuLSoF7YsPQKdwX5QYjZq66EsxWvNgLGQXlUrdZgKcWPz/dHb/sSsNt0YHo0dI8biEmxBv//TUJcc38MGVgDHKL9bjtg43Vvhf7xAQhq7AcRy4U4rFbI7Ev7RKOXCjEiO7hSGgRAEOFCbtT8vDsHc3x/bYUGE0CKw5kQK1SwNvNRQqmsU198OaQtsgp1qNMb8Tpi8VYvOccMgvLcUerQMSEesHdRY3/rTsBAPj44U7YlJwNd61aGh/UPMADZ3KutJj0ahmA1wa1gUatxLRlhy2C870dw/DnAfN/0Vd3F17Nx80FEf7u+L/hHXEhvwxpeaUY1CEUX285IzXpX02jVkqDH5+7owW8XNUW3Qk12TipN5bsO4dPN5wCYO7KM13+vbqe9rd4o1+bEJy7VIr0S6V4uldz9IkJwj9Hs7Bwd5pF1+L4PuZuA12FCWuPZsLHXYOH48KRW6LDvydy0O4Wb7y/Otnie66t09VCfVyRWVhepTv0y/90xSu/H6jSstKsiTveGNwWqbklMJqE9L78z60R+H/3dcC8bSl463LX0ANdmpq7Pqr5J+Hlfq3ww46zVVo1XuzTEp5aNVYczJC6zb7flnLdn9+ohEg0C/DAOyuOwkOjRlSgB0bGR2Da8iO1uoL1r8/cClcXFYZ+vg0A0KNFE5zNLYVSCYvXzt9Dg9uiA7D1ZA7ahnljRPcIvLBgHwDz377TF4ux6nAm3h7aTvqbWJMv/9MFz/28r8rzKTMHVRmvUV92Cxbx8fHo1q0bPvvsMwCAyWRCeHg4XnzxRUyZMsVmBaP6E0JY9cYqLDcgMSUPvaIDoFXXftBbVmE51EoFmtSxb2/H6Vy0CPJAkJcrLhbpEOCpqVJuk0lAqbTtL8m1inUVKCwzwNNVjV93pyO7qBw+bi44cK4Asx/qKA3+tEbllNqO4b7IKCjD/O2pGNg+FLFNfeDmooJKqYBCoUBabilu8XODSnml71RVy/qeyCrCN1vOYPxd0Qj3d8fG49lQKICYEG8EeGosApYQAhuTs3HmYgnimvmjU7gvhBD4ZXc6Qn1cEdfMr8qAvJr8vvccvth4CoNjQ1GiM6J7lB+8XV0Q37wJSvUVKNEZqx24Wp2rZzbllejR1M8cdq99zU0mAaMQFgMVLxbpcO5SKTpHXPnvbNeZXKw8lIEpA9tg4m9JOJCej29GxaFdmI/F8cr0Rjz01XZUGAWWje0pBWOFQoFiXQX+2HsOH6w+DnetGjOHdUCfmKAa34eHzxdAq1bipV+TcORCIf7btxVGdA/HjjO5SGjeBEHe5p/F1pM52HoqBy/2aYmUnBJsPnERu1Py8GSvKCxPOo9bfN0wsV8rAMCWkznw1KrRNdIPQgiYhPn3raDMgHnbUuCpdcHLd5sHGi/Zdw63Nm9SZYxIpXKDEf9bewJ3xgShWzP/Wr2/CssN+GTdSXi7uSAxNQ8j4yORX6rHH/vOITH1yqyvBU/Ho0eLJrhUaoCvmwuOZhRi/vZU3NEqEEM6hkEIgc82nML321LQOcIPMSFeeDyhmcX744G527H37CX8Oa4nYpv6Ql9hwpebzYMaB8eG4tylUrz111E80KUpukT44rutKXiiZxRCfFyRV6LHkn3nEB3shZcW7cdt0YGY/VDHKgN/swrLcSG/DF9sOo2+bYKQVahDclYR0nJLUVRuwE9PxSPc3x0FZQZ4adXSa/3vyYs4eK4A8VH+MAng+60puK9zGKICPNEq2BOrD2fCKATuiQ0DAHyz5QxMQuDZO1pYnD+joAzeri7VXuphzrqT0FUY8fLdrSGEQGZhOZr6uWPWquP49+RFfDKiM9YcycTt0YFITM3DB6uT0TbMG78/l4DXlx3Gsv3n0aNFE6w7lo0PHozF8LjwG76+1rJLsNDr9XB3d8fvv/+O++67T3p+1KhRyM/Px/Lly21WMCIiW7leMLU2gN/IuUul+PdkDh7q2tTmfdw3m6T0fOgrTOge5V/vY10q0SOrqBwxIfX7XLD163mzyi3WwV2jhpvmShgWQqCwvKJO/wTVRm0/v626QlZOTg6MRiOCg4Mtng8ODsbx49Uvn63T6aDTXWmiKix0zgAtImq8rtfaZesPoaZ+7hjRPcKmx7xZdQr3tdmx/Dw08Ltqqm1dNYZQAcCihbiyzgqFwm6hwhp2j9MzZ86Ej4+PdAsPt33zDBEREd0crAoWAQEBUKlUyMrKsng+KysLISEh1X7P1KlTUVBQIN3S09PrXloiIiK6qVkVLDQaDbp27Yr166+sZmkymbB+/XokJFRdiwEAtFotvL29LW5EREQkT1aNsQCAiRMnYtSoUYiLi0P37t3x8ccfo6SkBE888YQ9ykdEREQNiNXB4uGHH8bFixcxffp0ZGZmolOnTli9enWVAZ1ERETU+HBJbyIiIrqh2n5+y3uSNRERETkUgwURERHZDIMFERER2QyDBREREdkMgwURERHZDIMFERER2QyDBREREdmM1Qtk1Vflshm8yikREVHDUfm5faPlrxweLIqKigCAVzklIiJqgIqKiuDj41PjdoevvGkymXDhwgV4eXlJ15C3hcLCQoSHhyM9PV22K3rKvY6sX8Mn9zrKvX6A/Oso9/oB9qujEAJFRUUICwuDUlnzSAqHt1golUo0bdrUbsdvDFdQlXsdWb+GT+51lHv9APnXUe71A+xTx+u1VFTi4E0iIiKyGQYLIiIishnZBAutVos333wTWq3W2UWxG7nXkfVr+OReR7nXD5B/HeVeP8D5dXT44E0iIiKSL9m0WBAREZHzMVgQERGRzTBYEBERkc0wWBAREZHNyCZYfP7552jWrBlcXV0RHx+P3bt3O7tItbJlyxYMGTIEYWFhUCgUWLZsmcV2IQSmT5+O0NBQuLm5oW/fvjh58qTFPnl5eRg5ciS8vb3h6+uLp556CsXFxQ6sRc1mzpyJbt26wcvLC0FBQbjvvvuQnJxssU95eTnGjh2LJk2awNPTEw888ACysrIs9klLS8PgwYPh7u6OoKAgTJ48GRUVFY6sSrXmzp2L2NhYaSGahIQErFq1StrekOtWnVmzZkGhUOCll16SnmvodZwxYwYUCoXFLSYmRtre0OtX6fz58/jPf/6DJk2awM3NDR06dMCePXuk7Q35b02zZs2qvIYKhQJjx44F0PBfQ6PRiGnTpiEqKgpubm5o0aIF3nnnHYtrdtxUr5+QgUWLFgmNRiO+//57ceTIETFmzBjh6+srsrKynF20G1q5cqV4/fXXxZIlSwQAsXTpUovts2bNEj4+PmLZsmXiwIED4t577xVRUVGirKxM2mfAgAGiY8eOYufOneLff/8VLVu2FCNGjHBwTarXv39/MW/ePHH48GGRlJQkBg0aJCIiIkRxcbG0z3PPPSfCw8PF+vXrxZ49e8Stt94qevToIW2vqKgQ7du3F3379hX79+8XK1euFAEBAWLq1KnOqJKFP//8U/z999/ixIkTIjk5Wbz22mvCxcVFHD58WAjRsOt2rd27d4tmzZqJ2NhYMWHCBOn5hl7HN998U7Rr105kZGRIt4sXL0rbG3r9hBAiLy9PREZGitGjR4tdu3aJM2fOiDVr1ohTp05J+zTkvzXZ2dkWr9/atWsFALFx40YhRMN/Dd99913RpEkTsWLFCpGSkiIWL14sPD09xZw5c6R9bqbXTxbBonv37mLs2LHSY6PRKMLCwsTMmTOdWCrrXRssTCaTCAkJER9++KH0XH5+vtBqteKXX34RQghx9OhRAUAkJiZK+6xatUooFApx/vx5h5W9trKzswUAsXnzZiGEuT4uLi5i8eLF0j7Hjh0TAMSOHTuEEObwpVQqRWZmprTP3Llzhbe3t9DpdI6tQC34+fmJb7/9VlZ1KyoqEtHR0WLt2rXijjvukIKFHOr45ptvio4dO1a7TQ71E0KIV199VfTq1avG7XL7WzNhwgTRokULYTKZZPEaDh48WDz55JMWz91///1i5MiRQoib7/Vr8F0her0ee/fuRd++faXnlEol+vbtix07djixZPWXkpKCzMxMi7r5+PggPj5eqtuOHTvg6+uLuLg4aZ++fftCqVRi165dDi/zjRQUFAAA/P39AQB79+6FwWCwqGNMTAwiIiIs6tihQwcEBwdL+/Tv3x+FhYU4cuSIA0t/fUajEYsWLUJJSQkSEhJkVbexY8di8ODBFnUB5PP6nTx5EmFhYWjevDlGjhyJtLQ0APKp359//om4uDg89NBDCAoKQufOnfHNN99I2+X0t0av1+Pnn3/Gk08+CYVCIYvXsEePHli/fj1OnDgBADhw4AC2bt2KgQMHArj5Xj+HX4TM1nJycmA0Gi3eEAAQHByM48ePO6lUtpGZmQkA1datcltmZiaCgoIstqvVavj7+0v73CxMJhNeeukl9OzZE+3btwdgLr9Go4Gvr6/FvtfWsbqfQeU2Zzt06BASEhJQXl4OT09PLF26FG3btkVSUlKDrxsALFq0CPv27UNiYmKVbXJ4/eLj4zF//ny0bt0aGRkZeOutt3Dbbbfh8OHDsqgfAJw5cwZz587FxIkT8dprryExMRHjx4+HRqPBqFGjZPW3ZtmyZcjPz8fo0aMByOM9OmXKFBQWFiImJgYqlQpGoxHvvvsuRo4cCeDm+6xo8MGCGo6xY8fi8OHD2Lp1q7OLYlOtW7dGUlISCgoK8Pvvv2PUqFHYvHmzs4tlE+np6ZgwYQLWrl0LV1dXZxfHLir/6wOA2NhYxMfHIzIyEr/99hvc3NycWDLbMZlMiIuLw3vvvQcA6Ny5Mw4fPowvv/wSo0aNcnLpbOu7777DwIEDERYW5uyi2Mxvv/2GBQsWYOHChWjXrh2SkpLw0ksvISws7KZ8/Rp8V0hAQABUKlWVEb5ZWVkICQlxUqlso7L816tbSEgIsrOzLbZXVFQgLy/vpqr/uHHjsGLFCmzcuBFNmzaVng8JCYFer0d+fr7F/tfWsbqfQeU2Z9NoNGjZsiW6du2KmTNnomPHjpgzZ44s6rZ3715kZ2ejS5cuUKvVUKvV2Lx5Mz755BOo1WoEBwc3+Dpey9fXF61atcKpU6dk8RoCQGhoKNq2bWvxXJs2baQuH7n8rTl79izWrVuHp59+WnpODq/h5MmTMWXKFDzyyCPo0KEDHnvsMfz3v//FzJkzAdx8r1+DDxYajQZdu3bF+vXrpedMJhPWr1+PhIQEJ5as/qKiohASEmJRt8LCQuzatUuqW0JCAvLz87F3715pnw0bNsBkMiE+Pt7hZb6WEALjxo3D0qVLsWHDBkRFRVls79q1K1xcXCzqmJycjLS0NIs6Hjp0yOKXYu3atfD29q7yx/JmYDKZoNPpZFG3u+66C4cOHUJSUpJ0i4uLw8iRI6X7Db2O1youLsbp06cRGhoqi9cQAHr27FllmveJEycQGRkJQB5/awBg3rx5CAoKwuDBg6Xn5PAalpaWQqm0/LhWqVQwmUwAbsLXz6ZDQZ1k0aJFQqvVivnz54ujR4+KZ555Rvj6+lqM8L1ZFRUVif3794v9+/cLAOKjjz4S+/fvF2fPnhVCmKcQ+fr6iuXLl4uDBw+KoUOHVjuFqHPnzmLXrl1i69atIjo6+qaYAiaEEM8//7zw8fERmzZtspgOVlpaKu3z3HPPiYiICLFhwwaxZ88ekZCQIBISEqTtlVPB7r77bpGUlCRWr14tAgMDb4qpYFOmTBGbN28WKSkp4uDBg2LKlClCoVCIf/75RwjRsOtWk6tnhQjR8Ov48ssvi02bNomUlBSxbds20bdvXxEQECCys7OFEA2/fkKYpwqr1Wrx7rvvipMnT4oFCxYId3d38fPPP0v7NPS/NUajUURERIhXX321yraG/hqOGjVK3HLLLdJ00yVLloiAgADxyiuvSPvcTK+fLIKFEEJ8+umnIiIiQmg0GtG9e3exc+dOZxepVjZu3CgAVLmNGjVKCGGeRjRt2jQRHBwstFqtuOuuu0RycrLFMXJzc8WIESOEp6en8Pb2Fk888YQoKipyQm2qqq5uAMS8efOkfcrKysQLL7wg/Pz8hLu7uxg2bJjIyMiwOE5qaqoYOHCgcHNzEwEBAeLll18WBoPBwbWp6sknnxSRkZFCo9GIwMBAcdddd0mhQoiGXbeaXBssGnodH374YREaGio0Go245ZZbxMMPP2yxvkNDr1+lv/76S7Rv315otVoRExMjvv76a4vtDf1vzZo1awSAKmUWouG/hoWFhWLChAkiIiJCuLq6iubNm4vXX3/dYirszfT68bLpREREZDMNfowFERER3TwYLIiIiMhmGCyIiIjIZhgsiIiIyGYYLIiIiMhmGCyIiIjIZhgsiIiIyGYYLIiIiMhmGCyIiIjIZhgsiIiIyGYYLIiIiMhmGCyIiIjIZv4/wFunB67egY4AAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "GroundTruth_answers, finetuned_Predicted_answers = QA_generation(model, val_dataloader)"
      ],
      "metadata": {
        "id": "RrkKT0PL2CSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation of finetuned model"
      ],
      "metadata": {
        "id": "PhiyqB3ILw34"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge-score"
      ],
      "metadata": {
        "id": "41vzp9hQH-mQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_metric\n",
        "from statistics import mean\n",
        "\n",
        "def modified_bleu(scores: dict):\n",
        "\n",
        "  x = 1.0\n",
        "  for _ in scores['precisions'][:scores['reference_length']]:\n",
        "    x *= _\n",
        "\n",
        "  return scores['brevity_penalty'] * (x ** (1 / len(scores['precisions'][:scores['reference_length']])))\n",
        "\n",
        "\n",
        "def compute_evaluation_metrics(GroundTruth_answers,Predicted_answers):\n",
        "  bleu = load_metric(\"bleu\")\n",
        "  Avg_bleu_Score = mean([modified_bleu(bleu.compute(predictions=[b.split()], references=[[a.split()]])) for a,b in zip(GroundTruth_answers,Predicted_answers)])\n",
        "\n",
        "  rouge = load_metric(\"rouge\")\n",
        "  Avg_rouge_Score = mean([rouge.compute(use_aggregator=False, predictions=[b], references=[a])['rouge1'][0].fmeasure for a,b in zip(GroundTruth_answers,Predicted_answers)])\n",
        "\n",
        "  print(f\"Bleu-score: {Avg_bleu_Score}, ROUGE-score: {Avg_rouge_Score}\")\n",
        "\n",
        "\n",
        "\n",
        "def exact_match_score(GroundTruth_answers, Predicted_answers):\n",
        "\n",
        "  return sum([1 for a,b in zip(GroundTruth_answers, Predicted_answers) if a==b])\n"
      ],
      "metadata": {
        "id": "WH-JF4933TvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "QA performance from pretrained model"
      ],
      "metadata": {
        "id": "52zRWpMh2Rw2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "compute_evaluation_metrics(GroundTruth_answers, pretrained_Predicted_answers)"
      ],
      "metadata": {
        "id": "-LIauLARZcuo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22e808eb-58dc-439d-d295-ceb317a3f81c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bleu-score: 0.5508325421821997, ROUGE-score: 0.6718596525710846\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "QA performance from finetuned model"
      ],
      "metadata": {
        "id": "Ok5ESlRE2V7l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "compute_evaluation_metrics(GroundTruth_answers, finetuned_Predicted_answers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxQeGUOfhYCr",
        "outputId": "56656529-8652-45a1-f2b6-0bfe21326908"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bleu-score: 0.641538022737342, ROUGE-score: 0.7678881204793092\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparison in exact-match scores"
      ],
      "metadata": {
        "id": "7tVEI2FIJk4U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Before finetuning: {exact_match_score(GroundTruth_answers,pretrained_Predicted_answers)}, after finetuning: {exact_match_score(GroundTruth_answers,finetuned_Predicted_answers)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNIlFFV5zOQK",
        "outputId": "3b4c103a-11cb-4790-a882-179776e856f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before finetuning: 2436, after finetuning: 2882\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HnsogMeVJpf-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}